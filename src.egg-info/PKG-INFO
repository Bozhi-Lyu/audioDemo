Metadata-Version: 2.4
Name: src
Version: 0.0.1
Summary: A short description of the project.
Author-email: your@email.com
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: numpy>=1.26.0
Requires-Dist: pillow>=10.0.0
Requires-Dist: pip>=24.0
Requires-Dist: torch>=2.6.0
Requires-Dist: torch-summary>=1.4.5
Requires-Dist: torchaudio>=2.6.0
Requires-Dist: torchvision>=0.21.0
Requires-Dist: tqdm>=4.67.1
Provides-Extra: dev

# Audio Model Quantization

## Experiments setup

- Dataset: SpeechCommands ([Pytorch Link](https://pytorch.org/audio/main/generated/torchaudio.datasets.SPEECHCOMMANDS.html)).

- Model Architecture

    - CNN: M5 from [this paper](https://arxiv.org/abs/1610.00087).
    - Transformers: 
    - RNN-like # TODO


## Experiment Progress

|  | CNN | Transformer | RNN?(NSNet 2) |
| ---- | ---- | ---- | ---- |
| Full Precision (fp32) | [x] | [ ] | [ ] |
| Quantization Aware Training (QAT) | [x] | [ ] | [ ] |
| Post Training Quantization (PTQ) | [ ] | [ ] | [ ] |
| Activation-aware Weight Quantization (AWQ) | [ ] | [ ] | [ ] |

