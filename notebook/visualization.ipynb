{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Visualization on fp and quantized models\n",
    "\n",
    "## 1. The keys in the model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The FP model has 30 keys. The keys in FP model dict are: \n",
      "conv1.weight torch.float32 torch.Size([32, 1, 80])\n",
      "conv1.bias torch.float32 torch.Size([32])\n",
      "bn1.weight torch.float32 torch.Size([32])\n",
      "bn1.bias torch.float32 torch.Size([32])\n",
      "bn1.running_mean torch.float32 torch.Size([32])\n",
      "bn1.running_var torch.float32 torch.Size([32])\n",
      "bn1.num_batches_tracked torch.int64 torch.Size([])\n",
      "conv2.weight torch.float32 torch.Size([32, 32, 3])\n",
      "conv2.bias torch.float32 torch.Size([32])\n",
      "bn2.weight torch.float32 torch.Size([32])\n",
      "bn2.bias torch.float32 torch.Size([32])\n",
      "bn2.running_mean torch.float32 torch.Size([32])\n",
      "bn2.running_var torch.float32 torch.Size([32])\n",
      "bn2.num_batches_tracked torch.int64 torch.Size([])\n",
      "conv3.weight torch.float32 torch.Size([64, 32, 3])\n",
      "conv3.bias torch.float32 torch.Size([64])\n",
      "bn3.weight torch.float32 torch.Size([64])\n",
      "bn3.bias torch.float32 torch.Size([64])\n",
      "bn3.running_mean torch.float32 torch.Size([64])\n",
      "bn3.running_var torch.float32 torch.Size([64])\n",
      "bn3.num_batches_tracked torch.int64 torch.Size([])\n",
      "conv4.weight torch.float32 torch.Size([64, 64, 3])\n",
      "conv4.bias torch.float32 torch.Size([64])\n",
      "bn4.weight torch.float32 torch.Size([64])\n",
      "bn4.bias torch.float32 torch.Size([64])\n",
      "bn4.running_mean torch.float32 torch.Size([64])\n",
      "bn4.running_var torch.float32 torch.Size([64])\n",
      "bn4.num_batches_tracked torch.int64 torch.Size([])\n",
      "fc1.weight torch.float32 torch.Size([35, 64])\n",
      "fc1.bias torch.float32 torch.Size([35])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "fp_dict = torch.load(\"../models/fp32_model.pth\")\n",
    "print(\"The FP model has\", len(fp_dict.keys()), \"keys. The keys in FP model dict are: \")\n",
    "for key in fp_dict:\n",
    "    print(key, fp_dict[key].dtype, fp_dict[key].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quantized model has 22 keys. The keys in qat model dict are: \n",
      "conv1.weight torch.qint8 torch.Size([32, 1, 80])\n",
      "conv1.bias torch.float32 torch.Size([32])\n",
      "conv1.scale torch.float32 torch.Size([])\n",
      "conv1.zero_point torch.int64 torch.Size([])\n",
      "conv2.weight torch.qint8 torch.Size([32, 32, 3])\n",
      "conv2.bias torch.float32 torch.Size([32])\n",
      "conv2.scale torch.float32 torch.Size([])\n",
      "conv2.zero_point torch.int64 torch.Size([])\n",
      "conv3.weight torch.qint8 torch.Size([64, 32, 3])\n",
      "conv3.bias torch.float32 torch.Size([64])\n",
      "conv3.scale torch.float32 torch.Size([])\n",
      "conv3.zero_point torch.int64 torch.Size([])\n",
      "conv4.weight torch.qint8 torch.Size([64, 64, 3])\n",
      "conv4.bias torch.float32 torch.Size([64])\n",
      "conv4.scale torch.float32 torch.Size([])\n",
      "conv4.zero_point torch.int64 torch.Size([])\n",
      "fc1.scale torch.float32 torch.Size([])\n",
      "fc1.zero_point torch.int64 torch.Size([])\n",
      "fc1._packed_params.dtype <class 'torch.dtype'>\n",
      "fc1._packed_params._packed_params <class 'tuple'>\n",
      "quant.scale torch.float32 torch.Size([1])\n",
      "quant.zero_point torch.int64 torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "qat_dict = torch.load(\"../models/qat_model.pth\")\n",
    "print(\"The quantized model has\", len(qat_dict.keys()), \"keys. The keys in qat model dict are: \")\n",
    "for key in qat_dict:\n",
    "    value = qat_dict[key]\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(key, value.dtype, value.size())\n",
    "    else:\n",
    "        print(key, type(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys in the quantized model are different from the keys in the FP model. The difference is due to the following reasons:\n",
    "-  BatchNorm Folding: \n",
    "\n",
    "    --`bn.weight`, `bn.bias`, `running_mean`, `running_var`\n",
    "\n",
    "    During QAT, BatchNorm layers are fused with their corresponding `Conv1d` layers using `fuse_model()`. And the running statistics and batch norm parameters are folded into the `Conv1d` weights and biases. \n",
    "\n",
    "- Quantization Related Parameters: \n",
    "\n",
    "    ++`convX.scale`, `convX.zero_point`\n",
    "\n",
    "- FC Parameters Packing: \n",
    "\n",
    "    --`fc1.weight`, `fc1.bias` \n",
    "\n",
    "    ++`fc1._packed_params.dtype`, `fc1._packed_params`, `fc1.scale`, `fc1.zero_point`\n",
    "\n",
    "    Since the `nn.Linear` in fp model is replaced by `torch.ao.nn.qat.Linear` the quantized version, the keys of fc layers varied. \n",
    "\n",
    "    - `fc1._packed_params.dtype` stores the data type of the quantized weights in `fc1` (i.e. `torch.qint8`). \n",
    "    - `fc1._packed_params._packed_params` has 2 elements. The first one is quantized weight tensor, indicating its quantization scheme, scale and zero_point for each channel as well. The second one element is the bias tensor in float32. Usually the bias are not quantized.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.qint8\n",
      "torch.quantized.QInt8Tensor\n",
      "torch.Size([35, 64])\n",
      "tensor([[ 0.0344, -0.1279, -0.1894,  ...,  0.0000, -0.0271,  0.0025],\n",
      "        [-0.0525, -0.0739,  0.0447,  ...,  0.0000, -0.0311, -0.0097],\n",
      "        [-0.0202, -0.0173,  0.1080,  ...,  0.0000,  0.0086, -0.0043],\n",
      "        ...,\n",
      "        [-0.1384, -0.0336,  0.1285,  ...,  0.0000,  0.0040, -0.0040],\n",
      "        [-0.0579,  0.1221,  0.0150,  ...,  0.0000, -0.0450,  0.0000],\n",
      "        [ 0.1803,  0.0767,  0.0690,  ...,  0.0000, -0.0096,  0.0058]],\n",
      "       size=(35, 64), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0025, 0.0019, 0.0014, 0.0017, 0.0014, 0.0020, 0.0024, 0.0021, 0.0020,\n",
      "        0.0018, 0.0020, 0.0015, 0.0027, 0.0019, 0.0021, 0.0019, 0.0027, 0.0024,\n",
      "        0.0016, 0.0022, 0.0022, 0.0020, 0.0020, 0.0022, 0.0028, 0.0019, 0.0018,\n",
      "        0.0020, 0.0020, 0.0016, 0.0022, 0.0022, 0.0020, 0.0021, 0.0019],\n",
      "       dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
      "       axis=0)\n",
      "torch.float32\n",
      "torch.FloatTensor\n",
      "torch.Size([35])\n",
      "tensor(0.4217)\n",
      "tensor(72)\n"
     ]
    }
   ],
   "source": [
    "print(len(qat_dict[\"fc1._packed_params._packed_params\"]))\n",
    "print(qat_dict[\"fc1._packed_params._packed_params\"][0].dtype)\n",
    "print(qat_dict[\"fc1._packed_params._packed_params\"][0].type())\n",
    "print(qat_dict[\"fc1._packed_params._packed_params\"][0].size())\n",
    "print(qat_dict[\"fc1._packed_params._packed_params\"][0])\n",
    "\n",
    "print(qat_dict[\"fc1._packed_params._packed_params\"][1].dtype)\n",
    "print(qat_dict[\"fc1._packed_params._packed_params\"][1].type())\n",
    "print(qat_dict[\"fc1._packed_params._packed_params\"][1].size())\n",
    "\n",
    "print(qat_dict[\"fc1.scale\"])\n",
    "print(qat_dict[\"fc1.zero_point\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall\n",
    "| Components | FP model keys (30) | QAT model keys (22) |\n",
    "| --- | --- | --- |\n",
    "| Convolution layers | `convX.weight`, `convX.bias` | `convX.weight`, `convX.bias`, `convX.scale`, `convX.zero_point` |\n",
    "| BatchNorm layers | `bnX.weight`, `bnX.bias`, `bnX.running_mean`, `bnX.running_var` | Folded into `convX` |\n",
    "| FC layers | `fc1.weight`, `fc1.bias` | `fc1.scale`, `fc1.zero_point`, `fc1._packed_params.dtype`, `fc1._packed_params` |\n",
    "| Quant Stubs |  | `quant.scale`, `quant.zero_point` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
