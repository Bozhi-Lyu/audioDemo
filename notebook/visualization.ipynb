{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Visualization on fp and quantized models\n",
    "\n",
    "## 1. The keys in the model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The FP model has 30 keys. The keys in FP model dict are: \n",
      "conv1.weight torch.float32 torch.Size([32, 1, 80])\n",
      "conv1.bias torch.float32 torch.Size([32])\n",
      "bn1.weight torch.float32 torch.Size([32])\n",
      "bn1.bias torch.float32 torch.Size([32])\n",
      "bn1.running_mean torch.float32 torch.Size([32])\n",
      "bn1.running_var torch.float32 torch.Size([32])\n",
      "bn1.num_batches_tracked torch.int64 torch.Size([])\n",
      "conv2.weight torch.float32 torch.Size([32, 32, 3])\n",
      "conv2.bias torch.float32 torch.Size([32])\n",
      "bn2.weight torch.float32 torch.Size([32])\n",
      "bn2.bias torch.float32 torch.Size([32])\n",
      "bn2.running_mean torch.float32 torch.Size([32])\n",
      "bn2.running_var torch.float32 torch.Size([32])\n",
      "bn2.num_batches_tracked torch.int64 torch.Size([])\n",
      "conv3.weight torch.float32 torch.Size([64, 32, 3])\n",
      "conv3.bias torch.float32 torch.Size([64])\n",
      "bn3.weight torch.float32 torch.Size([64])\n",
      "bn3.bias torch.float32 torch.Size([64])\n",
      "bn3.running_mean torch.float32 torch.Size([64])\n",
      "bn3.running_var torch.float32 torch.Size([64])\n",
      "bn3.num_batches_tracked torch.int64 torch.Size([])\n",
      "conv4.weight torch.float32 torch.Size([64, 64, 3])\n",
      "conv4.bias torch.float32 torch.Size([64])\n",
      "bn4.weight torch.float32 torch.Size([64])\n",
      "bn4.bias torch.float32 torch.Size([64])\n",
      "bn4.running_mean torch.float32 torch.Size([64])\n",
      "bn4.running_var torch.float32 torch.Size([64])\n",
      "bn4.num_batches_tracked torch.int64 torch.Size([])\n",
      "fc1.weight torch.float32 torch.Size([35, 64])\n",
      "fc1.bias torch.float32 torch.Size([35])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "fp_dict = torch.load(\"../models/cnn_fp32_model.pth\")\n",
    "print(\"The FP model has\", len(fp_dict.keys()), \"keys. The keys in FP model dict are: \")\n",
    "for key in fp_dict:\n",
    "    print(key, fp_dict[key].dtype, fp_dict[key].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The QAT model has 22 keys. The keys in qat model dict are: \n",
      "conv1.weight torch.qint8 torch.Size([32, 1, 80])\n",
      "conv1.bias torch.float32 torch.Size([32])\n",
      "conv1.scale torch.float32 torch.Size([])\n",
      "conv1.zero_point torch.int64 torch.Size([])\n",
      "conv2.weight torch.qint8 torch.Size([32, 32, 3])\n",
      "conv2.bias torch.float32 torch.Size([32])\n",
      "conv2.scale torch.float32 torch.Size([])\n",
      "conv2.zero_point torch.int64 torch.Size([])\n",
      "conv3.weight torch.qint8 torch.Size([64, 32, 3])\n",
      "conv3.bias torch.float32 torch.Size([64])\n",
      "conv3.scale torch.float32 torch.Size([])\n",
      "conv3.zero_point torch.int64 torch.Size([])\n",
      "conv4.weight torch.qint8 torch.Size([64, 64, 3])\n",
      "conv4.bias torch.float32 torch.Size([64])\n",
      "conv4.scale torch.float32 torch.Size([])\n",
      "conv4.zero_point torch.int64 torch.Size([])\n",
      "fc1.scale torch.float32 torch.Size([])\n",
      "fc1.zero_point torch.int64 torch.Size([])\n",
      "fc1._packed_params.dtype <class 'torch.dtype'>\n",
      "fc1._packed_params._packed_params <class 'tuple'>\n",
      "quant.scale torch.float32 torch.Size([1])\n",
      "quant.zero_point torch.int64 torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/audioml/lib/python3.13/site-packages/torch/_utils.py:431: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    }
   ],
   "source": [
    "qat_dict = torch.load(\"../models/cnn_qat_model.pth\")\n",
    "print(\"The QAT model has\", len(qat_dict.keys()), \"keys. The keys in qat model dict are: \")\n",
    "for key in qat_dict:\n",
    "    value = qat_dict[key]\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(key, value.dtype, value.size())\n",
    "    else:\n",
    "        print(key, type(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PTQ model has 22 keys. The keys in PTQ model dict are: \n",
      "conv1.weight torch.qint8 torch.Size([32, 1, 80])\n",
      "conv1.bias torch.float32 torch.Size([32])\n",
      "conv1.scale torch.float32 torch.Size([])\n",
      "conv1.zero_point torch.int64 torch.Size([])\n",
      "conv2.weight torch.qint8 torch.Size([32, 32, 3])\n",
      "conv2.bias torch.float32 torch.Size([32])\n",
      "conv2.scale torch.float32 torch.Size([])\n",
      "conv2.zero_point torch.int64 torch.Size([])\n",
      "conv3.weight torch.qint8 torch.Size([64, 32, 3])\n",
      "conv3.bias torch.float32 torch.Size([64])\n",
      "conv3.scale torch.float32 torch.Size([])\n",
      "conv3.zero_point torch.int64 torch.Size([])\n",
      "conv4.weight torch.qint8 torch.Size([64, 64, 3])\n",
      "conv4.bias torch.float32 torch.Size([64])\n",
      "conv4.scale torch.float32 torch.Size([])\n",
      "conv4.zero_point torch.int64 torch.Size([])\n",
      "fc1.scale torch.float32 torch.Size([])\n",
      "fc1.zero_point torch.int64 torch.Size([])\n",
      "fc1._packed_params.dtype <class 'torch.dtype'>\n",
      "fc1._packed_params._packed_params <class 'tuple'>\n",
      "quant.scale torch.float32 torch.Size([1])\n",
      "quant.zero_point torch.int64 torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "ptq_dict = torch.load(\"../models/cnn_ptq_model.pth\")\n",
    "print(\"The PTQ model has\", len(ptq_dict.keys()), \"keys. The keys in PTQ model dict are: \")\n",
    "for key in ptq_dict:\n",
    "    value = ptq_dict[key]\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(key, value.dtype, value.size())\n",
    "    else:\n",
    "        print(key, type(value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys in the quantized model are different from the keys in the FP model. The difference is due to the following reasons:\n",
    "-  BatchNorm Folding: \n",
    "\n",
    "    --`bn.weight`, `bn.bias`, `running_mean`, `running_var`\n",
    "\n",
    "    During QAT, BatchNorm layers are fused with their corresponding `Conv1d` layers using `fuse_model()`. And the running statistics and batch norm parameters are folded into the `Conv1d` weights and biases. \n",
    "\n",
    "- Quantization Related Parameters: \n",
    "\n",
    "    ++`convX.scale`, `convX.zero_point`\n",
    "\n",
    "- FC Parameters Packing: \n",
    "\n",
    "    --`fc1.weight`, `fc1.bias` \n",
    "\n",
    "    ++`fc1._packed_params.dtype`, `fc1._packed_params`, `fc1.scale`, `fc1.zero_point`\n",
    "\n",
    "    Since the `nn.Linear` in fp model is replaced by `torch.ao.nn.qat.Linear` the quantized version, the keys of fc layers varied. \n",
    "\n",
    "    - `fc1._packed_params.dtype` stores the data type of the quantized weights in `fc1` (i.e. `torch.qint8`). \n",
    "    - `fc1._packed_params._packed_params` has 2 elements. The first one is quantized weight tensor, indicating its quantization scheme, scale and zero_point for each channel as well. The second one element is the bias tensor in float32. Usually the bias are not quantized.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "torch.qint8\n",
      "torch.qint8\n",
      "torch.quantized.QInt8Tensor\n",
      "torch.quantized.QInt8Tensor\n",
      "torch.Size([35, 64])\n",
      "torch.Size([35, 64])\n",
      "tensor([[ 0.0344, -0.1279, -0.1894,  ...,  0.0000, -0.0271,  0.0025],\n",
      "        [-0.0525, -0.0739,  0.0447,  ...,  0.0000, -0.0311, -0.0097],\n",
      "        [-0.0202, -0.0173,  0.1080,  ...,  0.0000,  0.0086, -0.0043],\n",
      "        ...,\n",
      "        [-0.1384, -0.0336,  0.1285,  ...,  0.0000,  0.0040, -0.0040],\n",
      "        [-0.0579,  0.1221,  0.0150,  ...,  0.0000, -0.0450,  0.0000],\n",
      "        [ 0.1803,  0.0767,  0.0690,  ...,  0.0000, -0.0096,  0.0058]],\n",
      "       size=(35, 64), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0025, 0.0019, 0.0014, 0.0017, 0.0014, 0.0020, 0.0024, 0.0021, 0.0020,\n",
      "        0.0018, 0.0020, 0.0015, 0.0027, 0.0019, 0.0021, 0.0019, 0.0027, 0.0024,\n",
      "        0.0016, 0.0022, 0.0022, 0.0020, 0.0020, 0.0022, 0.0028, 0.0019, 0.0018,\n",
      "        0.0020, 0.0020, 0.0016, 0.0022, 0.0022, 0.0020, 0.0021, 0.0019],\n",
      "       dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
      "       axis=0)\n",
      "tensor([[-0.0199, -0.2292, -0.1096,  ..., -0.2192, -0.0697, -0.0349],\n",
      "        [-0.2920, -0.3573, -0.2651,  ..., -0.1153, -0.3381, -0.1767],\n",
      "        [-0.2631, -0.3342, -0.2916,  ..., -0.1351,  0.0676,  0.0569],\n",
      "        ...,\n",
      "        [-0.1822,  0.2802, -0.1681,  ...,  0.1028, -0.0280,  0.0560],\n",
      "        [-0.0264,  0.2074,  0.0754,  ..., -0.2414, -0.2867,  0.4564],\n",
      "        [-0.2261,  0.2737,  0.2916,  ..., -0.2023, -0.2380, -0.0952]],\n",
      "       size=(35, 64), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0050, 0.0038, 0.0036, 0.0045, 0.0050, 0.0036, 0.0045, 0.0043, 0.0035,\n",
      "        0.0036, 0.0040, 0.0040, 0.0042, 0.0054, 0.0045, 0.0049, 0.0041, 0.0042,\n",
      "        0.0035, 0.0039, 0.0040, 0.0038, 0.0032, 0.0038, 0.0046, 0.0040, 0.0046,\n",
      "        0.0042, 0.0051, 0.0050, 0.0043, 0.0050, 0.0047, 0.0038, 0.0060],\n",
      "       dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
      "       axis=0)\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([35])\n",
      "torch.Size([35])\n",
      "tensor(0.4217)\n",
      "tensor(0.4778)\n",
      "tensor(72)\n",
      "tensor(78)\n"
     ]
    }
   ],
   "source": [
    "print(len(qat_dict[\"fc1._packed_params._packed_params\"]))\n",
    "print(len(ptq_dict[\"fc1._packed_params._packed_params\"]))\n",
    "\n",
    "print(qat_dict[\"fc1._packed_params._packed_params\"][0].dtype)\n",
    "print(ptq_dict[\"fc1._packed_params._packed_params\"][0].dtype)\n",
    "\n",
    "print(qat_dict[\"fc1._packed_params._packed_params\"][0].type())\n",
    "print(ptq_dict[\"fc1._packed_params._packed_params\"][0].type())\n",
    "\n",
    "print(qat_dict[\"fc1._packed_params._packed_params\"][0].size())\n",
    "print(ptq_dict[\"fc1._packed_params._packed_params\"][0].size())\n",
    "\n",
    "print(qat_dict[\"fc1._packed_params._packed_params\"][0])\n",
    "print(ptq_dict[\"fc1._packed_params._packed_params\"][0])\n",
    "\n",
    "print(qat_dict[\"fc1._packed_params._packed_params\"][1].dtype)\n",
    "print(ptq_dict[\"fc1._packed_params._packed_params\"][1].dtype)\n",
    "\n",
    "print(qat_dict[\"fc1._packed_params._packed_params\"][1].type())\n",
    "print(ptq_dict[\"fc1._packed_params._packed_params\"][1].type())\n",
    "\n",
    "print(qat_dict[\"fc1._packed_params._packed_params\"][1].size())\n",
    "print(ptq_dict[\"fc1._packed_params._packed_params\"][1].size())\n",
    "\n",
    "print(qat_dict[\"fc1.scale\"])\n",
    "print(ptq_dict[\"fc1.scale\"])\n",
    "\n",
    "print(qat_dict[\"fc1.zero_point\"])\n",
    "print(ptq_dict[\"fc1.zero_point\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall\n",
    "| Components | FP32 model keys (30) | QAT model keys (22) | PTQ model keys (22) |\n",
    "| --- | --- | --- | --- |\n",
    "| Convolution layers | `convX.weight`, `convX.bias` | `convX.weight`, `convX.bias`, `convX.scale`, `convX.zero_point` | same as QAT |\n",
    "| BatchNorm layers | `bnX.weight`, `bnX.bias`, `bnX.running_mean`, `bnX.running_var` | Folded into `convX` | same as QAT |\n",
    "| FC layers | `fc1.weight`, `fc1.bias` | `fc1.scale`, `fc1.zero_point`, `fc1._packed_params.dtype`, `fc1._packed_params` | same as QAT |\n",
    "| Quant Stubs |  | `quant.scale`, `quant.zero_point` | same as QAT |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results\n",
    "\n",
    "Run after comment `@profile`:\n",
    "\n",
    "```python\n",
    "\n",
    "python src/evaluate.py --checkpoint models/cnn_fp32_model.pth --config configs/cnn_fp32.yaml\n",
    "\n",
    "python src/evaluate.py --checkpoint models/cnn_qat_model.pth --config configs/cnn_qat.yaml\n",
    "\n",
    "python src/evaluate.py --checkpoint models/cnn_ptq_model.pth --config configs/cnn_ptq.yaml\n",
    "\n",
    "```\n",
    "\n",
    "| Metrics | FP32 | PTQ (Static) | QAT (from Checkpoint) |\n",
    "| --- | --- | --- | --- |\n",
    "| Model Size | 116.98 KB | 40.51 KB (2.89x) | ~~40.01 KB (≈2.9x)~~(TO BE UPDATED) |\n",
    "| Accuracy | 83.07% | 75.85% (↓8.69%) | ~~77.94% (↓4.37%)~~(TO BE UPDATED) |\n",
    "| Inference Time* | 15.744ms | 3.382ms (≈4.7x) | - | \n",
    "\n",
    "### 1. Accuracy\n",
    "\n",
    "Here I use the test_loader in `SpeechCommand` dataset:\n",
    "- Test loader length: 11005\n",
    "- Test loader batches: 43\n",
    "- Test loader batch size: 256\n",
    "\n",
    "### 2. Inference Time\n",
    "\n",
    "Here I use `line_profiler` to analyze the inference time in `test` function by: \n",
    "\n",
    "``` bash\n",
    "kernprof -l -v -o logs/profiling_logs/qat_profiling.lprof src/evaluate.py --checkpoint ./models/cnn_qat_model.pth --config ./configs/cnn_qat.yaml\n",
    "```\n",
    "\n",
    "And I got:\n",
    "\n",
    "```\n",
    "\n",
    "Timer unit: 1e-06 s\n",
    "Total time: 21.0433 s\n",
    "File: src/evaluate.py\n",
    "Function: test at line 52\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "    52                                           @profile\n",
    "    53                                           def test(model, test_loader):\n",
    "    54         1        175.0    175.0      0.0      model.eval()\n",
    "    55         1          0.1      0.1      0.0      correct = 0\n",
    "    56         2         32.2     16.1      0.0      with torch.no_grad():\n",
    "    57        44   20875165.1 474435.6     99.2          for data, target in test_loader:\n",
    "    58        43        483.3     11.2      0.0              data, target = data.to(device), target.to(device)\n",
    "    59        43     162906.2   *3788.5*      0.8              output = model(data)\n",
    "    60        43       2214.8     51.5      0.0              pred = get_likely_index(output)\n",
    "    61        43       2328.0     54.1      0.0              correct += number_of_correct(pred, target)\n",
    "    62         1          4.8      4.8      0.0      accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    63         1          0.7      0.7      0.0      return accuracy\n",
    "```\n",
    "\n",
    "\n",
    "Here I use the time per hit of line 59 as the average inference time, i.e. 3.788ms\n",
    "\n",
    "Likewise the result of fp32:\n",
    "\n",
    "``` bash\n",
    "kernprof -l -v -o logs/profiling_logs/fp32_profiling.lprof src/evaluate.py --checkpoint ./models/cnn_fp32_model.pth --config ./configs/cnn_fp32.yaml\n",
    "```\n",
    "\n",
    "And I got:\n",
    "\n",
    "```\n",
    "Timer unit: 1e-06 s\n",
    "Total time: 21.9024 s\n",
    "File: src/evaluate.py\n",
    "Function: test at line 52\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "    52                                           @profile\n",
    "    53                                           def test(model, test_loader):\n",
    "    54         1        165.0    165.0      0.0      model.eval()\n",
    "    55         1          0.2      0.2      0.0      correct = 0\n",
    "    56         2         26.0     13.0      0.0      with torch.no_grad():\n",
    "    57        44   21220067.8 482274.3     96.9          for data, target in test_loader:\n",
    "    58        43        558.4     13.0      0.0              data, target = data.to(device), target.to(device)\n",
    "    59        43     677012.1  *15744.5*      3.1              output = model(data)\n",
    "    60        43       2200.7     51.2      0.0              pred = get_likely_index(output)\n",
    "    61        43       2398.1     55.8      0.0              correct += number_of_correct(pred, target)\n",
    "    62         1          7.3      7.3      0.0      accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    63         1          0.8      0.8      0.0      return accuracy\n",
    "```\n",
    "\n",
    "and static quantization:\n",
    "\n",
    "``` bash \n",
    "kernprof -l -v -o logs/profiling_logs/ptq_profiling.lprof src/evaluate.py --checkpoint ./models/cnn_ptq_model.pth --config ./configs/cnn_ptq.yaml\n",
    "```\n",
    "\n",
    "and I got:\n",
    "\n",
    "```\n",
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 20.0763 s\n",
    "File: src/evaluate.py\n",
    "Function: test at line 52\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "    52                                           @profile\n",
    "    53                                           def test(model, test_loader):\n",
    "    54         1        177.1    177.1      0.0      model.eval()\n",
    "    55         1          0.2      0.2      0.0      correct = 0\n",
    "    56         2         34.7     17.4      0.0      with torch.no_grad():\n",
    "    57        44   19925933.9 452862.1     99.3          for data, target in test_loader:\n",
    "    58        43        494.9     11.5      0.0              data, target = data.to(device), target.to(device)\n",
    "    59        43     145440.8   3382.3      0.7              output = model(data)\n",
    "    60        43       2050.5     47.7      0.0              pred = get_likely_index(output)\n",
    "    61        43       2193.1     51.0      0.0              correct += number_of_correct(pred, target)\n",
    "    62         1          4.9      4.9      0.0      accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    63         1          0.7      0.7      0.0      return accuracy\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
