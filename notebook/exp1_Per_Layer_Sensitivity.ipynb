{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 Per-Layer Sensitivity\n",
    "\n",
    "The idea of this experienment Quantize only one block at a time, keep others FP32\n",
    "\n",
    "This experiment investigates the layer-wise sensitivity of a deep CNN model to quantization(PTQ, QAT). The goal is to understand how quantizing individual layers affects overall model accuracy, and to identify which layers are more robust or more sensitive to quantization-induced degradation. \n",
    "\n",
    "To achieve this, a modified, modularized variant of original deep CNN was implemented that allows selective quantization of individual convolutional blocks (L1–L4). For each run, only one block was quantized at a time while the rest remained in FP32. `qconfig` was applied only to the target block and `quant/dequant` stubs to isolate the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PTQ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/audioml/lib/python3.13/site-packages/torch/_utils.py:431: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from src.utils import *\n",
    "from src.data_loader import get_data_loaders\n",
    "from src.models.cnn_model_LayerWiseQuant import M5Modular, PTQM5Modular, PTQM5_LayerWiseQuant\n",
    "from src.evaluate import test\n",
    "fp_dict = torch.load(\"../models/cnn_fp32_model.pth\")\n",
    "ptq_dict = torch.load(\"../models/cnn_ptq_model.pth\")\n",
    "LWQ_dict_dicts = {\n",
    "    1: torch.load(\"../models/cnn_ptq_LayerWiseQuant_q1_model.pth\"),\n",
    "    2: torch.load(\"../models/cnn_ptq_LayerWiseQuant_q2_model.pth\"),\n",
    "    3: torch.load(\"../models/cnn_ptq_LayerWiseQuant_q3_model.pth\"),\n",
    "    4: torch.load(\"../models/cnn_ptq_LayerWiseQuant_q4_model.pth\"),\n",
    "}\n",
    "\n",
    "data_config = {\n",
    "    \"raw_dir\": \"../data/raw\",\n",
    "    \"processed_dir\": \"./data/processed\",\n",
    "    \"sample_rate\": 8000,\n",
    "    \"batch_size\": 256,\n",
    "    \"version\": \"v0.1\"\n",
    "}\n",
    "train_loader, test_loader, _ = get_data_loaders(data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 model accuracy: 83.0713\n"
     ]
    }
   ],
   "source": [
    "# Load FP model\n",
    "config_fp = '../configs/cnn_fp32.yaml'\n",
    "with open(config_fp, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "params_fp = config[\"model\"][\"base_cnn\"]\n",
    "model_fp = M5Modular(\n",
    "        n_input=params_fp[\"n_input\"],\n",
    "        n_output=params_fp[\"n_output\"],\n",
    "        stride=params_fp[\"stride\"],\n",
    "        n_channel=params_fp[\"n_channel\"],\n",
    "        conv_kernel_sizes=params_fp[\"conv_kernel_sizes\"]\n",
    "        )\n",
    "model_fp.load_state_dict(fp_dict)\n",
    "model_fp.to('cpu')\n",
    "\n",
    "# evaluate FP model\n",
    "acc_fp = test(model_fp, test_loader)\n",
    "print(f\"FP32 model accuracy: {acc_fp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTQ model accuracy: 75.8473\n"
     ]
    }
   ],
   "source": [
    "# Load fully quantized PTQ model\n",
    "# Load PTQ model\n",
    "config_PTQ = '../configs/cnn_ptq.yaml'\n",
    "with open(config_PTQ, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "params_PTQ = config[\"model\"][\"base_cnn\"]\n",
    "model_PTQ = PTQM5Modular(\n",
    "            n_input=params_PTQ[\"n_input\"],\n",
    "            n_output=params_PTQ[\"n_output\"],\n",
    "            stride=params_PTQ[\"stride\"],\n",
    "            n_channel=params_PTQ[\"n_channel\"],\n",
    "            conv_kernel_sizes=params_PTQ[\"conv_kernel_sizes\"]\n",
    "        )\n",
    "# Fuse and prepare for quantization\n",
    "model_PTQ.eval()\n",
    "model_PTQ.fuse_model()\n",
    "model_PTQ.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n",
    "\n",
    "model_PTQ.train()\n",
    "torch.ao.quantization.prepare_qat(model_PTQ, inplace=True)\n",
    "\n",
    "# Convert to quantized model\n",
    "model_PTQ.eval()\n",
    "model_PTQ = torch.ao.quantization.convert(model_PTQ, inplace=False)\n",
    "\n",
    "# Load checkpoint\n",
    "model_PTQ.load_state_dict(ptq_dict)\n",
    "model_PTQ.to('cpu')\n",
    "\n",
    "# evaluate PTQ model\n",
    "acc_PTQ = test(model_PTQ, test_loader)\n",
    "print(f\"PTQ model accuracy: {acc_PTQ:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer-Wise Quantized Model (Layer 1 quantized) accuracy: 76.8287\n",
      "Layer-Wise Quantized Model (Layer 2 quantized) accuracy: 80.8905\n",
      "Layer-Wise Quantized Model (Layer 3 quantized) accuracy: 81.8174\n",
      "Layer-Wise Quantized Model (Layer 4 quantized) accuracy: 82.9714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_LWQ = '../configs/cnn_ptq_LayerWiseQuant.yaml'\n",
    "with open(config_LWQ, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# for i in range(1, 2):\n",
    "for i in config[\"model\"][\"quantization\"]:\n",
    "    model_LWQ = PTQM5_LayerWiseQuant(\n",
    "        quantized_block_idx = i,\n",
    "        n_input=config[\"model\"][\"base_cnn\"][\"n_input\"],\n",
    "        n_output=config[\"model\"][\"base_cnn\"][\"n_output\"],\n",
    "        stride=config[\"model\"][\"base_cnn\"][\"stride\"],\n",
    "        n_channel=config[\"model\"][\"base_cnn\"][\"n_channel\"],\n",
    "        conv_kernel_sizes=config[\"model\"][\"base_cnn\"][\"conv_kernel_sizes\"],\n",
    "    )\n",
    "\n",
    "    # Fuse and prepare for quantization\n",
    "    model_LWQ.eval()\n",
    "    # print(f\"Layer-Wise Quantized Model before fuse: {model_LWQ}\")\n",
    "    model_LWQ.fuse_model()\n",
    "    # print(f\"Layer-Wise Quantized Model after fuse, before Layer {i} quantized: {model_LWQ}\")\n",
    "\n",
    "    qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "    model_LWQ.set_qconfig_for_layerwise(qconfig)\n",
    "    torch.ao.quantization.prepare(model_LWQ, inplace=True)\n",
    "\n",
    "    # Convert to quantized model\n",
    "    # model_LWQ.eval()\n",
    "    model_LWQ = torch.ao.quantization.convert(model_LWQ, inplace=False)\n",
    "    # print(f\"Layer-Wise Quantized Model Layer {i} quantized : {model_LWQ}\")\n",
    "    # # Load checkpoint\n",
    "    model_LWQ.load_state_dict(LWQ_dict_dicts[i])\n",
    "\n",
    "    # evaluate single layer quantized model\n",
    "    acc_LWQ = test(model_LWQ, test_loader)\n",
    "    print(f\"Layer-Wise Quantized Model (Layer {i} quantized) accuracy: {acc_LWQ:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Model|Acc|Accuracy Drop (vs. FP32)|\n",
    "|---|---|---|\n",
    "|FP32|83.0713|-0.00%|\n",
    "|PTQ (L4 Quantized)|82.9714|-0.10%|\n",
    "|PTQ (L3 Quantized)|81.8174|-1.25%|\n",
    "|PTQ (L2 Quantized)|80.8905|-2.18%|\n",
    "|PTQ (L1 Quantized)|76.8287|-6.24%|\n",
    "|PTQ (Fully Quantized)|75.8473|-7.22%|\n",
    "\n",
    "\n",
    "> Accuracy Drop=FP32 Accuracy−Quantized Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "1. Early layers (especially L1) are highly sensitive to quantization and significantly degrade accuracy when quantized. Later layers (L3, L4) are more robust. Early layers handle raw features, which are more sensitive to quantization noise. Later layers operate on higher-level representations and are more robust to quantization.\n",
    "\n",
    "2. Compared with L1-only quantization, fully quantized model reduces accuracy further, indicating accumulated quantization noise.\n",
    "\n",
    "3. Layer-wise PTQ provides insight into per-layer sensitivity, which can guide efficient mixed-precision or hybrid quantization strategies, e.g. If we choose mixed-precision, keep front layers in FP32 and quantize later layers; If applying QAT, prioritize front layers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTQ model accuracy drop L4: 0.099899999999991\n",
      "PTQ model accuracy drop L3: 1.2538999999999874\n",
      "PTQ model accuracy drop L2: 2.1807999999999907\n",
      "PTQ model accuracy drop L1: 6.242599999999996\n"
     ]
    }
   ],
   "source": [
    "x = (83.0713 - 82.9714) \n",
    "print(f\"PTQ model accuracy drop L4: {x}\")\n",
    "x = (83.0713 - 81.8174) \n",
    "print(f\"PTQ model accuracy drop L3: {x}\")\n",
    "x = (83.0713 - 80.8905) \n",
    "print(f\"PTQ model accuracy drop L2: {x}\")\n",
    "x = (83.0713 - 76.8287) \n",
    "print(f\"PTQ model accuracy drop L1: {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. QAT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
