{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 Per-Layer Sensitivity\n",
    "\n",
    "The idea of this experienment Quantize only one block at a time, keep others FP32.\n",
    "\n",
    "This experiment investigates the layer-wise sensitivity of a deep CNN model to quantization(PTQ, QAT). The goal is to understand how quantizing individual layers affects overall model accuracy, and to identify which layers are more robust or more sensitive to quantization-induced degradation. \n",
    "\n",
    "To achieve this, a modified, modularized variant of original deep CNN was implemented that allows selective quantization of individual convolutional blocks (L1–L4). For each run, only one block was quantized at a time while the rest remained in FP32. `qconfig` was applied only to the target block and `quant/dequant` stubs to isolate the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PTQ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_loader:  332\n",
      "Length of test_loader:  42\n",
      "Length of validate_loader:  38\n",
      "Shape of a batch in train_loader:  torch.Size([256, 1, 8000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from src.utils import *\n",
    "from src.data_loader import get_data_loaders\n",
    "from src.models.cnn_model_LayerWiseQuant import M5Modular, QATM5Modular, QATM5_LayerWiseQuant, PTQM5Modular, PTQM5_LayerWiseQuant\n",
    "from src.evaluate import test\n",
    "fp_dict = torch.load(\"../models/cnn_fp32_model.pth\")\n",
    "ptq_dict = torch.load(\"../models/cnn_ptq_model.pth\")\n",
    "LWQ_dict_dicts = {\n",
    "    1: torch.load(\"../models/cnn_ptq_LayerWiseQuant_q1_model.pth\"),\n",
    "    2: torch.load(\"../models/cnn_ptq_LayerWiseQuant_q2_model.pth\"),\n",
    "    3: torch.load(\"../models/cnn_ptq_LayerWiseQuant_q3_model.pth\"),\n",
    "    4: torch.load(\"../models/cnn_ptq_LayerWiseQuant_q4_model.pth\"),\n",
    "}\n",
    "\n",
    "data_config = {\n",
    "    \"raw_dir\": \"../data/raw\",\n",
    "    \"processed_dir\": \"../data/processed\",\n",
    "    \"sample_rate\": 8000,\n",
    "    \"batch_size\": 256,\n",
    "    \"version\": \"v0.1\"\n",
    "}\n",
    "train_loader, test_loader, validate_loader = get_data_loaders(data_config)\n",
    "# print(\"Length of train_loader: \", len(train_loader))\n",
    "# print(\"Length of test_loader: \", len(test_loader))\n",
    "# print(\"Length of validate_loader: \", len(validate_loader))\n",
    "# print(\"Shape of a batch in train_loader: \", next(iter(train_loader))[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the keys in the dictionaries\n",
    "LWPTQ_dict_sample = LWQ_dict_dicts[1]\n",
    "ptq_dict = torch.load(\"../models/cnn_ptq_model.pth\")\n",
    "print(\"Keys in fp_dict:\")\n",
    "for key in fp_dict.keys():\n",
    "    print(key)\n",
    "print(\"****************************************\")\n",
    "print(\"Keys in LWPTQ_dict_sample:\")\n",
    "for key in LWPTQ_dict_sample.keys():\n",
    "    print(key)\n",
    "print(\"****************************************\")\n",
    "print(\"Keys in ptq_dict:\")\n",
    "for key in ptq_dict.keys():\n",
    "    print(key)\n",
    "print(\"****************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ckeck keys in fp_dict after fusion\n",
    "fp_dict = torch.load(\"../models/cnn_fp32_model.pth\")\n",
    "with open('../configs/cnn_fp32.yaml') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "model = PTQM5Modular(\n",
    "    n_input=config[\"model\"][\"base_cnn\"][\"n_input\"],\n",
    "    n_output=config[\"model\"][\"base_cnn\"][\"n_output\"],\n",
    "    stride=config[\"model\"][\"base_cnn\"][\"stride\"],\n",
    "    n_channel=config[\"model\"][\"base_cnn\"][\"n_channel\"],\n",
    "    conv_kernel_sizes=config[\"model\"][\"base_cnn\"][\"conv_kernel_sizes\"]).to('cpu')\n",
    "model.eval()\n",
    "model.load_state_dict(fp_dict)\n",
    "print(\"Keys in fp_dict before fusion:\")\n",
    "for key in model.state_dict().keys(): \n",
    "    print(key)\n",
    "print(\"****************************************\")\n",
    "\n",
    "model.fuse_model() \n",
    "\n",
    "print(\"Keys in fp_dict after fusion:\")\n",
    "for key in model.state_dict().keys():\n",
    "    print(key)\n",
    "# Keys changed after fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check keys in qat_dict \n",
    "qat_dict = torch.load(\"../models/cnn_qat_model.pth\")\n",
    "print(\"Keys in qat_dict:\")\n",
    "for key in qat_dict.keys():\n",
    "    print(key)\n",
    "print(\"****************************************\")\n",
    "\n",
    "LWQAT_dict_sample = torch.load(\"../models/cnn_qat_LayerWiseQuant_q1_model.pth\")\n",
    "print(\"Keys in LWQAT_dict_sample:\")\n",
    "for key in LWQAT_dict_sample.keys():\n",
    "    print(key)\n",
    "print(\"****************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 model accuracy: 83.0713\n"
     ]
    }
   ],
   "source": [
    "# Load FP model\n",
    "config_fp = '../configs/cnn_fp32.yaml'\n",
    "with open(config_fp, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "params_fp = config[\"model\"][\"base_cnn\"]\n",
    "model_fp = M5Modular(\n",
    "        n_input=params_fp[\"n_input\"],\n",
    "        n_output=params_fp[\"n_output\"],\n",
    "        stride=params_fp[\"stride\"],\n",
    "        n_channel=params_fp[\"n_channel\"],\n",
    "        conv_kernel_sizes=params_fp[\"conv_kernel_sizes\"]\n",
    "        )\n",
    "model_fp.load_state_dict(fp_dict)\n",
    "model_fp.to('cpu')\n",
    "\n",
    "# evaluate FP model\n",
    "acc_fp = test(model_fp, test_loader)\n",
    "print(f\"FP32 model accuracy: {acc_fp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/audioml/lib/python3.13/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/audioml/lib/python3.13/site-packages/torch/ao/quantization/observer.py:1318: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTQ model accuracy: 75.8473\n"
     ]
    }
   ],
   "source": [
    "# Load fully quantized PTQ model\n",
    "# Load PTQ model\n",
    "config_PTQ = '../configs/cnn_ptq.yaml'\n",
    "with open(config_PTQ, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "params_PTQ = config[\"model\"][\"base_cnn\"]\n",
    "model_PTQ = PTQM5Modular(\n",
    "            n_input=params_PTQ[\"n_input\"],\n",
    "            n_output=params_PTQ[\"n_output\"],\n",
    "            stride=params_PTQ[\"stride\"],\n",
    "            n_channel=params_PTQ[\"n_channel\"],\n",
    "            conv_kernel_sizes=params_PTQ[\"conv_kernel_sizes\"]\n",
    "        )\n",
    "# Fuse and prepare for quantization\n",
    "model_PTQ.eval()\n",
    "model_PTQ.fuse_model()\n",
    "model_PTQ.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "\n",
    "model_PTQ.train()\n",
    "torch.ao.quantization.prepare_qat(model_PTQ, inplace=True)\n",
    "\n",
    "# Convert to quantized model\n",
    "model_PTQ.eval()\n",
    "model_PTQ = torch.ao.quantization.convert(model_PTQ, inplace=False)\n",
    "\n",
    "# Load checkpoint\n",
    "model_PTQ.load_state_dict(ptq_dict)\n",
    "model_PTQ.to('cpu')\n",
    "\n",
    "# evaluate PTQ model\n",
    "acc_PTQ = test(model_PTQ, test_loader)\n",
    "print(f\"PTQ model accuracy: {acc_PTQ:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer-Wise Quantized Model (Layer 1 quantized) accuracy: 76.8287\n",
      "Layer-Wise Quantized Model (Layer 2 quantized) accuracy: 80.8905\n",
      "Layer-Wise Quantized Model (Layer 3 quantized) accuracy: 81.8174\n",
      "Layer-Wise Quantized Model (Layer 4 quantized) accuracy: 82.9714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_LWQ = '../configs/cnn_ptq_LayerWiseQuant.yaml'\n",
    "with open(config_LWQ, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# for i in range(1, 2):\n",
    "for i in config[\"model\"][\"quantization\"]:\n",
    "    model_LWQ = PTQM5_LayerWiseQuant(\n",
    "        quantized_block_idx = i,\n",
    "        n_input=config[\"model\"][\"base_cnn\"][\"n_input\"],\n",
    "        n_output=config[\"model\"][\"base_cnn\"][\"n_output\"],\n",
    "        stride=config[\"model\"][\"base_cnn\"][\"stride\"],\n",
    "        n_channel=config[\"model\"][\"base_cnn\"][\"n_channel\"],\n",
    "        conv_kernel_sizes=config[\"model\"][\"base_cnn\"][\"conv_kernel_sizes\"],\n",
    "    )\n",
    "\n",
    "    # Fuse and prepare for quantization\n",
    "    model_LWQ.eval()\n",
    "    # print(f\"Layer-Wise Quantized Model before fuse: {model_LWQ}\")\n",
    "    model_LWQ.fuse_model()\n",
    "    # print(f\"Layer-Wise Quantized Model after fuse, before Layer {i} quantized: {model_LWQ}\")\n",
    "\n",
    "    qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "    model_LWQ.set_qconfig_for_layerwise(qconfig)\n",
    "    torch.ao.quantization.prepare(model_LWQ, inplace=True)\n",
    "\n",
    "    # Convert to quantized model\n",
    "    # model_LWQ.eval()\n",
    "    model_LWQ = torch.ao.quantization.convert(model_LWQ, inplace=False)\n",
    "    # print(f\"Layer-Wise Quantized Model Layer {i} quantized : {model_LWQ}\")\n",
    "    # # Load checkpoint\n",
    "    model_LWQ.load_state_dict(LWQ_dict_dicts[i])\n",
    "\n",
    "    # evaluate single layer quantized model\n",
    "    acc_LWQ = test(model_LWQ, test_loader)\n",
    "    print(f\"Layer-Wise Quantized Model (Layer {i} quantized) accuracy: {acc_LWQ:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Model|Acc|Accuracy Drop (vs. FP32)|\n",
    "|---|---|---|\n",
    "|FP32|83.0713|-0.00%|\n",
    "|PTQ (L4 Quantized)|82.9714|-0.10%|\n",
    "|PTQ (L3 Quantized)|81.8174|-1.25%|\n",
    "|PTQ (L2 Quantized)|80.8905|-2.18%|\n",
    "|PTQ (L1 Quantized)|76.8287|-6.24%|\n",
    "|PTQ (Fully Quantized)|75.8473|-7.22%|\n",
    "\n",
    "\n",
    "> Accuracy Drop=FP32 Accuracy−Quantized Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights\n",
    "\n",
    "1. Early layers (especially L1) are highly sensitive to quantization and significantly degrade accuracy when quantized. Later layers (L3, L4) are more robust. Early layers handle raw features, which are more sensitive to quantization noise. Later layers operate on higher-level representations and are more robust to quantization.\n",
    "\n",
    "2. Compared with L1-only quantization, fully quantized model reduces accuracy further, indicating accumulated quantization noise.\n",
    "\n",
    "3. Layer-wise PTQ provides insight into per-layer sensitivity, which can guide efficient mixed-precision or hybrid quantization strategies, e.g. If we choose mixed-precision, keep front layers in FP32 and quantize later layers; If applying QAT, prioritize front layers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (83.0713 - 82.9714) \n",
    "print(f\"PTQ model accuracy drop L4: {x}\")\n",
    "x = (83.0713 - 81.8174) \n",
    "print(f\"PTQ model accuracy drop L3: {x}\")\n",
    "x = (83.0713 - 80.8905) \n",
    "print(f\"PTQ model accuracy drop L2: {x}\")\n",
    "x = (83.0713 - 76.8287) \n",
    "print(f\"PTQ model accuracy drop L1: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a new combined checkpoint dictionary from layer-wise PTQ models\n",
    "combined_dict = {}\n",
    "\n",
    "# Define mapping of block keys to their source dict\n",
    "for i in range(1, 5):\n",
    "    prefix = f\"block{i}.block.0\"\n",
    "    for suffix in [\"weight\", \"bias\", \"scale\", \"zero_point\"]:\n",
    "        key = f\"{prefix}.{suffix}\"\n",
    "        combined_dict[key] = LWQ_dict_dicts[i][key]\n",
    "\n",
    "# Add remaining non-block keys (e.g., fc1, quant/dequant) from fully quantized model\n",
    "for key in ptq_dict:\n",
    "    if not any(key.startswith(f\"block{i}.block.0\") for i in range(1, 5)):\n",
    "        combined_dict[key] = ptq_dict[key]\n",
    "        \n",
    "# print(\"Keys in combined_dict:\")\n",
    "# for key in combined_dict.keys():\n",
    "#     print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/audioml/lib/python3.13/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/audioml/lib/python3.13/site-packages/torch/ao/quantization/observer.py:1318: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'fc1.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m model_PTQ_combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mao\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mconvert(model_PTQ_combined, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Load checkpoint\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmodel_PTQ_combined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m model_PTQ_combined\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# evaluate PTQ model\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/audioml/lib/python3.13/site-packages/torch/nn/modules/module.py:2561\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2554\u001b[0m         out \u001b[38;5;241m=\u001b[39m hook(module, incompatible_keys)\n\u001b[1;32m   2555\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m   2556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2557\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2558\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit should be done inplace.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2559\u001b[0m         )\n\u001b[0;32m-> 2561\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m load\n\u001b[1;32m   2564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "File \u001b[0;32m~/miniconda3/envs/audioml/lib/python3.13/site-packages/torch/nn/modules/module.py:2549\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2543\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2544\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2545\u001b[0m             k: v\n\u001b[1;32m   2546\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2547\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)\n\u001b[1;32m   2548\u001b[0m         }\n\u001b[0;32m-> 2549\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m   2551\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2552\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[0;32m~/miniconda3/envs/audioml/lib/python3.13/site-packages/torch/nn/modules/module.py:2532\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m assign:\n\u001b[1;32m   2531\u001b[0m     local_metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massign_to_params_buffers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m assign\n\u001b[0;32m-> 2532\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2536\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2538\u001b[0m \u001b[43m    \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2539\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_msgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/audioml/lib/python3.13/site-packages/torch/ao/nn/quantized/modules/linear.py:251\u001b[0m, in \u001b[0;36mLinear._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m    247\u001b[0m version \u001b[38;5;241m=\u001b[39m local_metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m version \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# We moved the parameters into a LinearPackedParameters submodule\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     bias \u001b[38;5;241m=\u001b[39m state_dict\u001b[38;5;241m.\u001b[39mpop(prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    253\u001b[0m     state_dict\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    254\u001b[0m         {\n\u001b[1;32m    255\u001b[0m             prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_packed_params.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: weight,\n\u001b[1;32m    256\u001b[0m             prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_packed_params.bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: bias,\n\u001b[1;32m    257\u001b[0m         }\n\u001b[1;32m    258\u001b[0m     )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'fc1.weight'"
     ]
    }
   ],
   "source": [
    "model_PTQ_combined = PTQM5Modular(\n",
    "            n_input=params_PTQ[\"n_input\"],\n",
    "            n_output=params_PTQ[\"n_output\"],\n",
    "            stride=params_PTQ[\"stride\"],\n",
    "            n_channel=params_PTQ[\"n_channel\"],\n",
    "            conv_kernel_sizes=params_PTQ[\"conv_kernel_sizes\"]\n",
    "        )\n",
    "# Fuse and prepare for quantization\n",
    "model_PTQ_combined.eval()\n",
    "model_PTQ_combined.fuse_model()\n",
    "model_PTQ_combined.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "\n",
    "model_PTQ_combined.train()\n",
    "torch.ao.quantization.prepare(model_PTQ_combined, inplace=True)\n",
    "\n",
    "# Convert to quantized model\n",
    "model_PTQ_combined.eval()\n",
    "model_PTQ_combined = torch.ao.quantization.convert(model_PTQ_combined, inplace=False)\n",
    "\n",
    "# Load checkpoint\n",
    "model_PTQ_combined.load_state_dict(combined_dict)\n",
    "model_PTQ_combined.to('cpu')\n",
    "\n",
    "# evaluate PTQ model\n",
    "acc_PTQ = test(model_PTQ_combined, test_loader)\n",
    "print(f\"Combined PTQ model accuracy: {acc_PTQ:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fully quantized QAT model\n",
    "# Load QAT model\n",
    "config_QAT = '../configs/cnn_qat.yaml'\n",
    "with open(config_QAT, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "params_QAT = config[\"model\"][\"base_cnn\"]\n",
    "model_QAT = QATM5Modular(\n",
    "            n_input=params_QAT[\"n_input\"],\n",
    "            n_output=params_QAT[\"n_output\"],\n",
    "            stride=params_QAT[\"stride\"],\n",
    "            n_channel=params_QAT[\"n_channel\"],\n",
    "            conv_kernel_sizes=params_QAT[\"conv_kernel_sizes\"]\n",
    "        )\n",
    "# Fuse and prepare for quantization\n",
    "model_QAT.eval()\n",
    "model_QAT.fuse_model()\n",
    "model_QAT.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n",
    "\n",
    "model_QAT.train()\n",
    "torch.ao.quantization.prepare_qat(model_QAT, inplace=True)\n",
    "\n",
    "# Convert to quantized model\n",
    "model_QAT.eval()\n",
    "model_QAT = torch.ao.quantization.convert(model_QAT, inplace=False)\n",
    "\n",
    "# Load checkpoint\n",
    "qat_dict = torch.load(\"../models/cnn_qat_model.pth\")\n",
    "model_QAT.load_state_dict(qat_dict)\n",
    "model_QAT.to('cpu')\n",
    "\n",
    "# evaluate QAT model\n",
    "acc_QAT = test(model_QAT, test_loader)\n",
    "print(f\"QAT model accuracy: {acc_QAT:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_LWQ = '../configs/cnn_qat_LayerWiseQuant.yaml'\n",
    "with open(config_LWQ, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "LWQ_QAT_dict_dicts = {\n",
    "    1: torch.load(\"../models/cnn_qat_LayerWiseQuant_q1_model.pth\"),\n",
    "    2: torch.load(\"../models/cnn_qat_LayerWiseQuant_q2_model.pth\"),\n",
    "    3: torch.load(\"../models/cnn_qat_LayerWiseQuant_q3_model.pth\"),\n",
    "    4: torch.load(\"../models/cnn_qat_LayerWiseQuant_q4_model.pth\"),\n",
    "}\n",
    "\n",
    "for i in config[\"model\"][\"quantization\"]:\n",
    "    model_LWQ = QATM5_LayerWiseQuant(\n",
    "        quantized_block_idx = i,\n",
    "        n_input=config[\"model\"][\"base_cnn\"][\"n_input\"],\n",
    "        n_output=config[\"model\"][\"base_cnn\"][\"n_output\"],\n",
    "        stride=config[\"model\"][\"base_cnn\"][\"stride\"],\n",
    "        n_channel=config[\"model\"][\"base_cnn\"][\"n_channel\"],\n",
    "        conv_kernel_sizes=config[\"model\"][\"base_cnn\"][\"conv_kernel_sizes\"],\n",
    "    )\n",
    "\n",
    "    # Fuse and prepare for quantization\n",
    "    model_LWQ.eval()\n",
    "    # print(f\"Layer-Wise Quantized Model before fuse: {model_LWQ}\")\n",
    "    model_LWQ.fuse_model()\n",
    "    # print(f\"Layer-Wise Quantized Model after fuse, before Layer {i} quantized: {model_LWQ}\")\n",
    "\n",
    "    qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n",
    "    model_LWQ.set_qconfig_for_layerwise(qconfig)\n",
    "    torch.ao.quantization.prepare(model_LWQ, inplace=True)\n",
    "\n",
    "    # Convert to quantized model\n",
    "    # model_LWQ.eval()\n",
    "    model_LWQ = torch.ao.quantization.convert(model_LWQ, inplace=False)\n",
    "    # print(f\"Layer-Wise Quantized Model Layer {i} quantized : {model_LWQ}\")\n",
    "    # # Load checkpoint\n",
    "    model_LWQ.load_state_dict(LWQ_QAT_dict_dicts[i])\n",
    "\n",
    "    # evaluate single layer quantized model\n",
    "    acc_LWQ = test(model_LWQ, test_loader)\n",
    "    print(f\"Layer-Wise Quantized Model (Layer {i} quantized) accuracy: {acc_LWQ:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Model|Acc|Accuracy Drop (vs. FP32)|\n",
    "|---|---|---|\n",
    "|FP32|83.0713|-0.00%|\n",
    "|**QAT (L4 Quantized)**|83.2894|+0.22%(??)|\n",
    "|PTQ (L4 Quantized)|82.9714|-0.10%|\n",
    "|**QAT (L3 Quantized)**|82.7442|-0.33%|\n",
    "|PTQ (L3 Quantized)|81.8174|-1.25%|\n",
    "|**QAT (L2 Quantized)**|81.2631|-1.81%|\n",
    "|PTQ (L2 Quantized)|80.8905|-2.18%|\n",
    "|**QAT (L1 Quantized)**|80.4180|-2.65%|\n",
    "|PTQ (L1 Quantized)|76.8287|-6.24%|\n",
    "|**QAT (Fully Quantized)**|79.4639|-3.61%|\n",
    "|PTQ (Fully Quantized)|75.8473|-7.22%|\n",
    "\n",
    "> Accuracy Drop=FP32 Accuracy−Quantized Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (83.0713 - 83.2894) \n",
    "print(f\"QAT model accuracy drop L4: {x}\")\n",
    "x = (83.0713 - 82.7442) \n",
    "print(f\"QAT model accuracy drop L3: {x}\")\n",
    "x = (83.0713 - 81.2631) \n",
    "print(f\"QAT model accuracy drop L2: {x}\")\n",
    "x = (83.0713 - 80.4180) \n",
    "print(f\"QAT model accuracy drop L1: {x}\")\n",
    "x = (83.0713 - 79.4639) \n",
    "print(f\"QAT model accuracy drop: {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights\n",
    "\n",
    "1. QAT consistently outperforms PTQ\n",
    "\n",
    "2. Early layers (e.g., L1) are most sensitive to quantization\n",
    "\n",
    "\n",
    "| Layer | PTQ Drop | QAT Drop   |\n",
    "| ----- | -------- | ---------- |\n",
    "| L1    | -6.24%   | -2.65%     |\n",
    "| L4    | -0.10%   | **+0.22%** |\n",
    "\n",
    "\n",
    "3.Later layers are robust — QAT even improves L4(???)\n",
    "\n",
    "- QAT on L4 gives +0.22% gain over FP32.\n",
    "- This small improvement may be due to full precision of FC layer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
