{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 Per-Layer Sensitivity\n",
    "\n",
    "The idea of this experienment Quantize only one block at a time, keep others FP32.\n",
    "\n",
    "This experiment investigates the layer-wise sensitivity of a deep CNN model to quantization(PTQ, QAT). The goal is to understand how quantizing individual layers affects overall model accuracy, and to identify which layers are more robust or more sensitive to quantization-induced degradation. \n",
    "\n",
    "To achieve this, a modified, modularized variant of original deep CNN was implemented that allows selective quantization of individual convolutional blocks (L1–L4). For each run, only one block was quantized at a time while the rest remained in FP32. `qconfig` was applied only to the target block and `quant/dequant` stubs to isolate the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PTQ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/audioml/lib/python3.13/site-packages/torch/_utils.py:431: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from src.utils import *\n",
    "from src.data_loader import get_data_loaders\n",
    "from src.models.cnn_model_LayerWiseQuant import M5Modular, QATM5Modular, QATM5_LayerWiseQuant, PTQM5Modular, PTQM5_LayerWiseQuant\n",
    "from src.evaluate import test\n",
    "fp_dict = torch.load(\"../models/cnn_fp32_model.pth\")\n",
    "ptq_dict = torch.load(\"../models/cnn_ptq_model.pth\")\n",
    "LWQ_dict_dicts = {\n",
    "    1: torch.load(\"../models/cnn_ptq_LayerWiseQuant_q1_model.pth\"),\n",
    "    2: torch.load(\"../models/cnn_ptq_LayerWiseQuant_q2_model.pth\"),\n",
    "    3: torch.load(\"../models/cnn_ptq_LayerWiseQuant_q3_model.pth\"),\n",
    "    4: torch.load(\"../models/cnn_ptq_LayerWiseQuant_q4_model.pth\"),\n",
    "}\n",
    "\n",
    "data_config = {\n",
    "    \"raw_dir\": \"../data/raw\",\n",
    "    \"processed_dir\": \"../data/processed\",\n",
    "    \"sample_rate\": 8000,\n",
    "    \"batch_size\": 256,\n",
    "    \"version\": \"v0.1\"\n",
    "}\n",
    "train_loader, test_loader, validate_loader = get_data_loaders(data_config)\n",
    "# print(\"Length of train_loader: \", len(train_loader))\n",
    "# print(\"Length of test_loader: \", len(test_loader))\n",
    "# print(\"Length of validate_loader: \", len(validate_loader))\n",
    "# print(\"Shape of a batch in train_loader: \", next(iter(train_loader))[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in fp_dict:\n",
      "block1.block.0.weight\n",
      "block1.block.0.bias\n",
      "block1.block.1.weight\n",
      "block1.block.1.bias\n",
      "block1.block.1.running_mean\n",
      "block1.block.1.running_var\n",
      "block1.block.1.num_batches_tracked\n",
      "block2.block.0.weight\n",
      "block2.block.0.bias\n",
      "block2.block.1.weight\n",
      "block2.block.1.bias\n",
      "block2.block.1.running_mean\n",
      "block2.block.1.running_var\n",
      "block2.block.1.num_batches_tracked\n",
      "block3.block.0.weight\n",
      "block3.block.0.bias\n",
      "block3.block.1.weight\n",
      "block3.block.1.bias\n",
      "block3.block.1.running_mean\n",
      "block3.block.1.running_var\n",
      "block3.block.1.num_batches_tracked\n",
      "block4.block.0.weight\n",
      "block4.block.0.bias\n",
      "block4.block.1.weight\n",
      "block4.block.1.bias\n",
      "block4.block.1.running_mean\n",
      "block4.block.1.running_var\n",
      "block4.block.1.num_batches_tracked\n",
      "fc1.weight\n",
      "fc1.bias\n",
      "****************************************\n",
      "Keys in LWPTQ_dict_sample:\n",
      "block1.block.0.weight\n",
      "block1.block.0.bias\n",
      "block1.block.0.scale\n",
      "block1.block.0.zero_point\n",
      "block2.block.0.0.weight\n",
      "block2.block.0.0.bias\n",
      "block3.block.0.0.weight\n",
      "block3.block.0.0.bias\n",
      "block4.block.0.0.weight\n",
      "block4.block.0.0.bias\n",
      "fc1.weight\n",
      "fc1.bias\n",
      "quant.scale\n",
      "quant.zero_point\n",
      "****************************************\n",
      "Keys in ptq_dict:\n",
      "block1.block.0.weight\n",
      "block1.block.0.bias\n",
      "block1.block.0.scale\n",
      "block1.block.0.zero_point\n",
      "block2.block.0.weight\n",
      "block2.block.0.bias\n",
      "block2.block.0.scale\n",
      "block2.block.0.zero_point\n",
      "block3.block.0.weight\n",
      "block3.block.0.bias\n",
      "block3.block.0.scale\n",
      "block3.block.0.zero_point\n",
      "block4.block.0.weight\n",
      "block4.block.0.bias\n",
      "block4.block.0.scale\n",
      "block4.block.0.zero_point\n",
      "fc1.scale\n",
      "fc1.zero_point\n",
      "fc1._packed_params.dtype\n",
      "fc1._packed_params._packed_params\n",
      "quant.scale\n",
      "quant.zero_point\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "# Check the keys in the dictionaries\n",
    "LWPTQ_dict_sample = LWQ_dict_dicts[1]\n",
    "ptq_dict = torch.load(\"../models/cnn_ptq_model.pth\")\n",
    "print(\"Keys in fp_dict:\")\n",
    "for key in fp_dict.keys():\n",
    "    print(key)\n",
    "print(\"****************************************\")\n",
    "print(\"Keys in LWPTQ_dict_sample:\")\n",
    "for key in LWPTQ_dict_sample.keys():\n",
    "    print(key)\n",
    "print(\"****************************************\")\n",
    "print(\"Keys in ptq_dict:\")\n",
    "for key in ptq_dict.keys():\n",
    "    print(key)\n",
    "print(\"****************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in fp_dict before fusion:\n",
      "block1.block.0.weight\n",
      "block1.block.0.bias\n",
      "block1.block.1.weight\n",
      "block1.block.1.bias\n",
      "block1.block.1.running_mean\n",
      "block1.block.1.running_var\n",
      "block1.block.1.num_batches_tracked\n",
      "block2.block.0.weight\n",
      "block2.block.0.bias\n",
      "block2.block.1.weight\n",
      "block2.block.1.bias\n",
      "block2.block.1.running_mean\n",
      "block2.block.1.running_var\n",
      "block2.block.1.num_batches_tracked\n",
      "block3.block.0.weight\n",
      "block3.block.0.bias\n",
      "block3.block.1.weight\n",
      "block3.block.1.bias\n",
      "block3.block.1.running_mean\n",
      "block3.block.1.running_var\n",
      "block3.block.1.num_batches_tracked\n",
      "block4.block.0.weight\n",
      "block4.block.0.bias\n",
      "block4.block.1.weight\n",
      "block4.block.1.bias\n",
      "block4.block.1.running_mean\n",
      "block4.block.1.running_var\n",
      "block4.block.1.num_batches_tracked\n",
      "fc1.weight\n",
      "fc1.bias\n",
      "****************************************\n",
      "Keys in fp_dict after fusion:\n",
      "block1.block.0.0.weight\n",
      "block1.block.0.0.bias\n",
      "block2.block.0.0.weight\n",
      "block2.block.0.0.bias\n",
      "block3.block.0.0.weight\n",
      "block3.block.0.0.bias\n",
      "block4.block.0.0.weight\n",
      "block4.block.0.0.bias\n",
      "fc1.weight\n",
      "fc1.bias\n"
     ]
    }
   ],
   "source": [
    "# Ckeck keys in fp_dict after fusion\n",
    "fp_dict = torch.load(\"../models/cnn_fp32_model.pth\")\n",
    "with open('../configs/cnn_fp32.yaml') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "model = PTQM5Modular(\n",
    "    n_input=config[\"model\"][\"base_cnn\"][\"n_input\"],\n",
    "    n_output=config[\"model\"][\"base_cnn\"][\"n_output\"],\n",
    "    stride=config[\"model\"][\"base_cnn\"][\"stride\"],\n",
    "    n_channel=config[\"model\"][\"base_cnn\"][\"n_channel\"],\n",
    "    conv_kernel_sizes=config[\"model\"][\"base_cnn\"][\"conv_kernel_sizes\"]).to('cpu')\n",
    "model.eval()\n",
    "model.load_state_dict(fp_dict)\n",
    "print(\"Keys in fp_dict before fusion:\")\n",
    "for key in model.state_dict().keys(): \n",
    "    print(key)\n",
    "print(\"****************************************\")\n",
    "\n",
    "model.fuse_model() \n",
    "\n",
    "print(\"Keys in fp_dict after fusion:\")\n",
    "for key in model.state_dict().keys():\n",
    "    print(key)\n",
    "# Keys changed after fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in qat_dict:\n",
      "block1.block.0.weight\n",
      "block1.block.0.bias\n",
      "block1.block.0.scale\n",
      "block1.block.0.zero_point\n",
      "block2.block.0.weight\n",
      "block2.block.0.bias\n",
      "block2.block.0.scale\n",
      "block2.block.0.zero_point\n",
      "block3.block.0.weight\n",
      "block3.block.0.bias\n",
      "block3.block.0.scale\n",
      "block3.block.0.zero_point\n",
      "block4.block.0.weight\n",
      "block4.block.0.bias\n",
      "block4.block.0.scale\n",
      "block4.block.0.zero_point\n",
      "fc1.scale\n",
      "fc1.zero_point\n",
      "fc1._packed_params.dtype\n",
      "fc1._packed_params._packed_params\n",
      "quant.scale\n",
      "quant.zero_point\n",
      "****************************************\n",
      "Keys in LWQAT_dict_sample:\n",
      "block1.block.0.weight\n",
      "block1.block.0.bias\n",
      "block1.block.0.scale\n",
      "block1.block.0.zero_point\n",
      "block2.block.0.0.weight\n",
      "block2.block.0.0.bias\n",
      "block3.block.0.0.weight\n",
      "block3.block.0.0.bias\n",
      "block4.block.0.0.weight\n",
      "block4.block.0.0.bias\n",
      "fc1.weight\n",
      "fc1.bias\n",
      "quant.scale\n",
      "quant.zero_point\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "# Check keys in qat_dict \n",
    "qat_dict = torch.load(\"../models/cnn_qat_model.pth\")\n",
    "print(\"Keys in qat_dict:\")\n",
    "for key in qat_dict.keys():\n",
    "    print(key)\n",
    "print(\"****************************************\")\n",
    "\n",
    "LWQAT_dict_sample = torch.load(\"../models/cnn_qat_LayerWiseQuant_q1_model.pth\")\n",
    "print(\"Keys in LWQAT_dict_sample:\")\n",
    "for key in LWQAT_dict_sample.keys():\n",
    "    print(key)\n",
    "print(\"****************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 model accuracy: 83.0713\n"
     ]
    }
   ],
   "source": [
    "# Load FP model\n",
    "config_fp = '../configs/cnn_fp32.yaml'\n",
    "with open(config_fp, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "params_fp = config[\"model\"][\"base_cnn\"]\n",
    "model_fp = M5Modular(\n",
    "        n_input=params_fp[\"n_input\"],\n",
    "        n_output=params_fp[\"n_output\"],\n",
    "        stride=params_fp[\"stride\"],\n",
    "        n_channel=params_fp[\"n_channel\"],\n",
    "        conv_kernel_sizes=params_fp[\"conv_kernel_sizes\"]\n",
    "        )\n",
    "model_fp.load_state_dict(fp_dict)\n",
    "model_fp.to('cpu')\n",
    "\n",
    "# evaluate FP model\n",
    "acc_fp = test(model_fp, test_loader)\n",
    "print(f\"FP32 model accuracy: {acc_fp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/audioml/lib/python3.13/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/audioml/lib/python3.13/site-packages/torch/ao/quantization/observer.py:1318: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTQ model accuracy: 75.8473\n"
     ]
    }
   ],
   "source": [
    "# Load fully quantized PTQ model\n",
    "# Load PTQ model\n",
    "config_PTQ = '../configs/cnn_ptq.yaml'\n",
    "with open(config_PTQ, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "params_PTQ = config[\"model\"][\"base_cnn\"]\n",
    "model_PTQ = PTQM5Modular(\n",
    "            n_input=params_PTQ[\"n_input\"],\n",
    "            n_output=params_PTQ[\"n_output\"],\n",
    "            stride=params_PTQ[\"stride\"],\n",
    "            n_channel=params_PTQ[\"n_channel\"],\n",
    "            conv_kernel_sizes=params_PTQ[\"conv_kernel_sizes\"]\n",
    "        )\n",
    "# Fuse and prepare for quantization\n",
    "model_PTQ.eval()\n",
    "model_PTQ.fuse_model()\n",
    "model_PTQ.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "\n",
    "model_PTQ.train()\n",
    "torch.ao.quantization.prepare_qat(model_PTQ, inplace=True)\n",
    "\n",
    "# Convert to quantized model\n",
    "model_PTQ.eval()\n",
    "model_PTQ = torch.ao.quantization.convert(model_PTQ, inplace=False)\n",
    "\n",
    "# Load checkpoint\n",
    "model_PTQ.load_state_dict(ptq_dict)\n",
    "model_PTQ.to('cpu')\n",
    "\n",
    "# evaluate PTQ model\n",
    "acc_PTQ = test(model_PTQ, test_loader)\n",
    "print(f\"PTQ model accuracy: {acc_PTQ:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer-Wise Quantized Model (Layer 1 quantized) accuracy: 76.8287\n",
      "Layer-Wise Quantized Model (Layer 2 quantized) accuracy: 80.8905\n",
      "Layer-Wise Quantized Model (Layer 3 quantized) accuracy: 81.8174\n",
      "Layer-Wise Quantized Model (Layer 4 quantized) accuracy: 82.9714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_LWQ = '../configs/cnn_ptq_LayerWiseQuant.yaml'\n",
    "with open(config_LWQ, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# for i in range(1, 2):\n",
    "for i in config[\"model\"][\"quantization\"]:\n",
    "    model_LWQ = PTQM5_LayerWiseQuant(\n",
    "        quantized_block_idx = i,\n",
    "        n_input=config[\"model\"][\"base_cnn\"][\"n_input\"],\n",
    "        n_output=config[\"model\"][\"base_cnn\"][\"n_output\"],\n",
    "        stride=config[\"model\"][\"base_cnn\"][\"stride\"],\n",
    "        n_channel=config[\"model\"][\"base_cnn\"][\"n_channel\"],\n",
    "        conv_kernel_sizes=config[\"model\"][\"base_cnn\"][\"conv_kernel_sizes\"],\n",
    "    )\n",
    "\n",
    "    # Fuse and prepare for quantization\n",
    "    model_LWQ.eval()\n",
    "    # print(f\"Layer-Wise Quantized Model before fuse: {model_LWQ}\")\n",
    "    model_LWQ.fuse_model()\n",
    "    # print(f\"Layer-Wise Quantized Model after fuse, before Layer {i} quantized: {model_LWQ}\")\n",
    "\n",
    "    qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "    model_LWQ.set_qconfig_for_layerwise(qconfig)\n",
    "    torch.ao.quantization.prepare(model_LWQ, inplace=True)\n",
    "\n",
    "    # Convert to quantized model\n",
    "    # model_LWQ.eval()\n",
    "    model_LWQ = torch.ao.quantization.convert(model_LWQ, inplace=False)\n",
    "    # print(f\"Layer-Wise Quantized Model Layer {i} quantized : {model_LWQ}\")\n",
    "    # # Load checkpoint\n",
    "    model_LWQ.load_state_dict(LWQ_dict_dicts[i])\n",
    "\n",
    "    # evaluate single layer quantized model\n",
    "    acc_LWQ = test(model_LWQ, test_loader)\n",
    "    print(f\"Layer-Wise Quantized Model (Layer {i} quantized) accuracy: {acc_LWQ:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Model|Acc|Accuracy Drop (vs. FP32)|\n",
    "|---|---|---|\n",
    "|FP32|83.0713|-0.00%|\n",
    "|PTQ (L4 Quantized)|82.9714|-0.10%|\n",
    "|PTQ (L3 Quantized)|81.8174|-1.25%|\n",
    "|PTQ (L2 Quantized)|80.8905|-2.18%|\n",
    "|PTQ (L1 Quantized)|76.8287|-6.24%|\n",
    "|PTQ (Fully Quantized)|75.8473|-7.22%|\n",
    "\n",
    "\n",
    "> Accuracy Drop=FP32 Accuracy−Quantized Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights\n",
    "\n",
    "1. Early layers (especially L1) are highly sensitive to quantization and significantly degrade accuracy when quantized. Later layers (L3, L4) are more robust. Early layers handle raw features, which are more sensitive to quantization noise. Later layers operate on higher-level representations and are more robust to quantization.\n",
    "\n",
    "2. Compared with L1-only quantization, fully quantized model reduces accuracy further, indicating accumulated quantization noise.\n",
    "\n",
    "3. Layer-wise PTQ provides insight into per-layer sensitivity, which can guide efficient mixed-precision or hybrid quantization strategies, e.g. If we choose mixed-precision, keep front layers in FP32 and quantize later layers; If applying QAT, prioritize front layers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (83.0713 - 82.9714) \n",
    "print(f\"PTQ model accuracy drop L4: {x}\")\n",
    "x = (83.0713 - 81.8174) \n",
    "print(f\"PTQ model accuracy drop L3: {x}\")\n",
    "x = (83.0713 - 80.8905) \n",
    "print(f\"PTQ model accuracy drop L2: {x}\")\n",
    "x = (83.0713 - 76.8287) \n",
    "print(f\"PTQ model accuracy drop L1: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a new combined checkpoint dictionary from layer-wise PTQ models\n",
    "combined_dict = {}\n",
    "\n",
    "# Define mapping of block keys to their source dict\n",
    "for i in range(1, 5):\n",
    "    prefix = f\"block{i}.block.0\"\n",
    "    for suffix in [\"weight\", \"bias\", \"scale\", \"zero_point\"]:\n",
    "        key = f\"{prefix}.{suffix}\"\n",
    "        combined_dict[key] = LWQ_dict_dicts[i][key]\n",
    "\n",
    "# Add remaining non-block keys (e.g., fc1, quant/dequant) from fully quantized model\n",
    "for key in ptq_dict:\n",
    "    if not any(key.startswith(f\"block{i}.block.0\") for i in range(1, 5)):\n",
    "        combined_dict[key] = ptq_dict[key]\n",
    "        \n",
    "# print(\"Keys in combined_dict:\")\n",
    "# for key in combined_dict.keys():\n",
    "#     print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PTQ_combined = PTQM5Modular(\n",
    "            n_input=params_PTQ[\"n_input\"],\n",
    "            n_output=params_PTQ[\"n_output\"],\n",
    "            stride=params_PTQ[\"stride\"],\n",
    "            n_channel=params_PTQ[\"n_channel\"],\n",
    "            conv_kernel_sizes=params_PTQ[\"conv_kernel_sizes\"]\n",
    "        )\n",
    "# Fuse and prepare for quantization\n",
    "model_PTQ_combined.eval()\n",
    "model_PTQ_combined.fuse_model()\n",
    "model_PTQ_combined.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "\n",
    "model_PTQ_combined.train()\n",
    "torch.ao.quantization.prepare(model_PTQ_combined, inplace=True)\n",
    "\n",
    "# Convert to quantized model\n",
    "model_PTQ_combined.eval()\n",
    "model_PTQ_combined = torch.ao.quantization.convert(model_PTQ_combined, inplace=False)\n",
    "\n",
    "# Load checkpoint\n",
    "model_PTQ_combined.load_state_dict(combined_dict)\n",
    "model_PTQ_combined.to('cpu')\n",
    "\n",
    "# evaluate PTQ model\n",
    "acc_PTQ = test(model_PTQ_combined, test_loader)\n",
    "print(f\"Combined PTQ model accuracy: {acc_PTQ:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fully quantized QAT model\n",
    "# Load QAT model\n",
    "config_QAT = '../configs/cnn_qat.yaml'\n",
    "with open(config_QAT, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "params_QAT = config[\"model\"][\"base_cnn\"]\n",
    "model_QAT = QATM5Modular(\n",
    "            n_input=params_QAT[\"n_input\"],\n",
    "            n_output=params_QAT[\"n_output\"],\n",
    "            stride=params_QAT[\"stride\"],\n",
    "            n_channel=params_QAT[\"n_channel\"],\n",
    "            conv_kernel_sizes=params_QAT[\"conv_kernel_sizes\"]\n",
    "        )\n",
    "# Fuse and prepare for quantization\n",
    "model_QAT.eval()\n",
    "model_QAT.fuse_model()\n",
    "model_QAT.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n",
    "\n",
    "model_QAT.train()\n",
    "torch.ao.quantization.prepare_qat(model_QAT, inplace=True)\n",
    "\n",
    "# Convert to quantized model\n",
    "model_QAT.eval()\n",
    "model_QAT = torch.ao.quantization.convert(model_QAT, inplace=False)\n",
    "\n",
    "# Load checkpoint\n",
    "qat_dict = torch.load(\"../models/cnn_qat_model.pth\")\n",
    "model_QAT.load_state_dict(qat_dict)\n",
    "model_QAT.to('cpu')\n",
    "\n",
    "# evaluate QAT model\n",
    "acc_QAT = test(model_QAT, test_loader)\n",
    "print(f\"QAT model accuracy: {acc_QAT:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_LWQ = '../configs/cnn_qat_LayerWiseQuant.yaml'\n",
    "with open(config_LWQ, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "LWQ_QAT_dict_dicts = {\n",
    "    1: torch.load(\"../models/cnn_qat_LayerWiseQuant_q1_model.pth\"),\n",
    "    2: torch.load(\"../models/cnn_qat_LayerWiseQuant_q2_model.pth\"),\n",
    "    3: torch.load(\"../models/cnn_qat_LayerWiseQuant_q3_model.pth\"),\n",
    "    4: torch.load(\"../models/cnn_qat_LayerWiseQuant_q4_model.pth\"),\n",
    "}\n",
    "\n",
    "for i in config[\"model\"][\"quantization\"]:\n",
    "    model_LWQ = QATM5_LayerWiseQuant(\n",
    "        quantized_block_idx = i,\n",
    "        n_input=config[\"model\"][\"base_cnn\"][\"n_input\"],\n",
    "        n_output=config[\"model\"][\"base_cnn\"][\"n_output\"],\n",
    "        stride=config[\"model\"][\"base_cnn\"][\"stride\"],\n",
    "        n_channel=config[\"model\"][\"base_cnn\"][\"n_channel\"],\n",
    "        conv_kernel_sizes=config[\"model\"][\"base_cnn\"][\"conv_kernel_sizes\"],\n",
    "    )\n",
    "\n",
    "    # Fuse and prepare for quantization\n",
    "    model_LWQ.eval()\n",
    "    # print(f\"Layer-Wise Quantized Model before fuse: {model_LWQ}\")\n",
    "    model_LWQ.fuse_model()\n",
    "    # print(f\"Layer-Wise Quantized Model after fuse, before Layer {i} quantized: {model_LWQ}\")\n",
    "\n",
    "    qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n",
    "    model_LWQ.set_qconfig_for_layerwise(qconfig)\n",
    "    torch.ao.quantization.prepare(model_LWQ, inplace=True)\n",
    "\n",
    "    # Convert to quantized model\n",
    "    # model_LWQ.eval()\n",
    "    model_LWQ = torch.ao.quantization.convert(model_LWQ, inplace=False)\n",
    "    # print(f\"Layer-Wise Quantized Model Layer {i} quantized : {model_LWQ}\")\n",
    "    # # Load checkpoint\n",
    "    model_LWQ.load_state_dict(LWQ_QAT_dict_dicts[i])\n",
    "\n",
    "    # evaluate single layer quantized model\n",
    "    acc_LWQ = test(model_LWQ, test_loader)\n",
    "    print(f\"Layer-Wise Quantized Model (Layer {i} quantized) accuracy: {acc_LWQ:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Model|Acc|Accuracy Drop (vs. FP32)|\n",
    "|---|---|---|\n",
    "|FP32|83.0713|-0.00%|\n",
    "|**QAT (L4 Quantized)**|83.2894|+0.22%(??)|\n",
    "|PTQ (L4 Quantized)|82.9714|-0.10%|\n",
    "|**QAT (L3 Quantized)**|82.7442|-0.33%|\n",
    "|PTQ (L3 Quantized)|81.8174|-1.25%|\n",
    "|**QAT (L2 Quantized)**|81.2631|-1.81%|\n",
    "|PTQ (L2 Quantized)|80.8905|-2.18%|\n",
    "|**QAT (L1 Quantized)**|80.4180|-2.65%|\n",
    "|PTQ (L1 Quantized)|76.8287|-6.24%|\n",
    "|**QAT (Fully Quantized)**|79.4639|-3.61%|\n",
    "|PTQ (Fully Quantized)|75.8473|-7.22%|\n",
    "\n",
    "> Accuracy Drop=FP32 Accuracy−Quantized Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (83.0713 - 83.2894) \n",
    "print(f\"QAT model accuracy drop L4: {x}\")\n",
    "x = (83.0713 - 82.7442) \n",
    "print(f\"QAT model accuracy drop L3: {x}\")\n",
    "x = (83.0713 - 81.2631) \n",
    "print(f\"QAT model accuracy drop L2: {x}\")\n",
    "x = (83.0713 - 80.4180) \n",
    "print(f\"QAT model accuracy drop L1: {x}\")\n",
    "x = (83.0713 - 79.4639) \n",
    "print(f\"QAT model accuracy drop: {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights\n",
    "\n",
    "1. QAT consistently outperforms PTQ\n",
    "\n",
    "2. Early layers (e.g., L1) are most sensitive to quantization\n",
    "\n",
    "\n",
    "| Layer | PTQ Drop | QAT Drop   |\n",
    "| ----- | -------- | ---------- |\n",
    "| L1    | -6.24%   | -2.65%     |\n",
    "| L4    | -0.10%   | **+0.22%** |\n",
    "\n",
    "\n",
    "3.Later layers are robust — QAT even improves L4(???)\n",
    "\n",
    "- QAT on L4 gives +0.22% gain over FP32.\n",
    "- This small improvement may be due to full precision of FC layer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
