{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio and Quantization Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts of Audio\n",
    "\n",
    "- waveforms\n",
    "\n",
    "\n",
    "- sample rate\n",
    "\n",
    "    The sampling frequency or sampling rate, $f_s$, is the average number of samples obtained in one second, thus $f_{s}=1/T$, with the unit samples per second, sometimes referred to as hertz, for example 48 kHz is 48,000 samples per second.\n",
    "\n",
    "- spectrograms\n",
    "    A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time.\n",
    "\n",
    "- MFC and MFCCs\n",
    "\n",
    "    - Mel-frequency cepstrum (MFC): a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.\n",
    "\n",
    "    - Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC.\n",
    "\n",
    "    MFCCs are commonly derived as follows:\n",
    "\n",
    "    - Take the Fourier transform of a waveform (frame).\n",
    "    - Map the powers of the spectrum obtained above onto the mel scale, using triangular overlapping windows or alternatively, cosine overlapping windows.\n",
    "    - Take the logs of the powers at each of the mel frequencies.\n",
    "    - Take the discrete cosine transform of the list of mel log powers, as if it were a signal.\n",
    "    - The MFCCs are the amplitudes of the resulting spectrum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Basic\n",
    "\n",
    "### Reference\n",
    "\n",
    "- [Introduction to Quantization on PyTorch](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/)\n",
    "\n",
    "- [Quantization for Neural Networks - Lei Mao](https://leimao.github.io/article/Neural-Networks-Quantization/)\n",
    "\n",
    "- [Quantization - Pytorch Doc](https://pytorch.org/docs/stable/quantization.html)\n",
    "\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported Quantization types by Pytorch:\n",
    "\n",
    "- Post training dynamic quantization \n",
    "\n",
    "Quantizes both weights and activations, but calibrates activation ranges dynamically at runtime (per input).\n",
    "\n",
    "    - No need for calibration data (ranges computed during inference).\n",
    "    - Better latency improvement than weight-only.\n",
    "\n",
    "- Post training static quantization \n",
    "\n",
    "Quantizes weights and activations using pre-calibrated ranges (fixed scales/zero-points).\n",
    "\n",
    "    - Calibration: Requires a representative dataset to compute optimal quantization ranges for activations.\n",
    "\n",
    "- static quantization aware training (weights quantized, activations quantized, quantization numerics modeled during training)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
