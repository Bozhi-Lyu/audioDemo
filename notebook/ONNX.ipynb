{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes of Exporting Models to ONNX format\n",
    "\n",
    "## 1. Before Quantization\n",
    "\n",
    "### 1.1 ONNX-Compatible Modification Before Exporting\n",
    "\n",
    "Model architecture should be modified to be ONNX-compatible. Related code:\n",
    "\n",
    "```python\n",
    "x = F.avg_pool1d(x, x.shape[-1]) \n",
    "```\n",
    "\n",
    "->\n",
    "\n",
    "```python\n",
    "x = F.adaptive_avg_pool1d(x, 1)\n",
    "```\n",
    "- `F.avg_pool1d(x, x.shape[-1])` uses a dynamic kernel size and PyTorch evaluates it at runtime. ONNX export uses static tracing (`torch.onnx.export()`), and it cannot trace dynamic kernel sizes derived from input shapes.\n",
    "- `adaptive_avg_pool1d` is natively supported in ONNX and symbolically defines the output to always have a fixed length (here, 1).\n",
    "\n",
    "```python \n",
    "x = x.permute(0, 2, 1)\n",
    "```\n",
    "\n",
    "->\n",
    "```python\n",
    "x = x.transpose(1, 2)\n",
    "```\n",
    "- The permute() operation is not supported in the quantized version of PyTorch. [Related Issue Here.](https://github.com/pytorch/pytorch/issues/109425)\n",
    "- This is because of the symbolic function, which describe how to map a PyTorch op to an ONNX op, of `permute` assumes float tensors only. As shown in the [official symbolic_opset](https://github.com/pytorch/pytorch/blob/ffaed8c569406839335bf46dafc4c3e8871e4b8a/torch/onnx/symbolic_opset9.py#L989), `@symbolic_helper.quantized_args(True)` didn't decorate the symbolic function `def permute`.\n",
    "- Since the `aten.permute` has been implemented [here](https://docs.pytorch.org/docs/main/torch.compiler_ir.html), the straightforward way I've tried is to overwrite and registry the symbolic function of `permute` by `torch.onnx.symbolic_registry.register_op`, but it turned to be very complicated. \n",
    "- Then I found the symbolic function of `transpose` is supported for quantized version of the base operator, so here's the tricky way.\n",
    "\n",
    "\n",
    "\n",
    "### 1.2 Preprocess by `onnxruntime.quantization.preprocess` \\(Quantization in ort)\n",
    "\n",
    "``` bash\n",
    "python -m onnxruntime.quantization.preprocess \\\n",
    "    --input models/cnn_fp32.onnx \\\n",
    "    --output models/cnn_fp32_infer.onnx\n",
    "```\n",
    "Pre-processing is to transform a float32 model to prepare it for quantization and improve quantization quality. It consists of the following three optional steps:\n",
    "\n",
    "- Symbolic shape inference. This is best suited for transformer models.\n",
    "- ONNX shape inference.\n",
    "- Model optimization: This step uses ONNX Runtime native library to rewrite the computation graph, including merging computation nodes, eliminating redundancies to improve runtime efficiency.\n",
    "\n",
    "In our case, according to the computational graph, the preprocessing helps to:\n",
    "- Figures out the shape in each step in the graph (The shape is noted next to each arrow after preprocessing);\n",
    "- Fuse `Matmul` and `Add` operators into [`Gemm`](https://onnx.ai/onnx/operators/onnx__Gemm.html) operator for matrix multiplication.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Precision Alignment before and after exporting\n",
    "\n",
    "[Reference](https://web.mit.edu/10.001/Web/Tips/Converge.htm)\n",
    "\n",
    "I use `numpy.testing.assert_allclose` for precision alignment before and after exporting to ONNX. \n",
    "\n",
    "- For FP models, I set tolerances as `rtol=1e-3, atol=1e-05`and it passed the test. \n",
    "\n",
    "- For Quantized model I set `rtol=0.1, atol=0.1`and it failed, and only ~95%(QAT) and ~97%(PTQ) top-1 predictions of pytorch model and ONNX model matched. However the final accuracy keep almost the same.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation after Quantization\n",
    "Run the script `./onnx_static_quantize.sh` of overall workflow of static quantization in ONNX Runtime and evaluation, given by the well-trained fp32 model checkpoint. Run `./onnx_qat_quantize.sh` to export QAT model to ONNX as well. The output will be close to this:\n",
    "\n",
    "|Metrics|FP32 ONNX|Int8 ONNX|QAT Pytorch|QAT ONNX|PTQ Pytorch|PTQ ONNX|\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| Model Size | 106.51 KB | 44.09 KB | 40.62 KB | 46.18 KB | 40.62 KB | 46.18 KB |\n",
    "| Accuracy | 83.14% | 78.41% | 79.63% | 79.73% | 75.85% | 75.89% |\n",
    "| Average Inference Time | 9.62 ms | 2.85 ms | \\- |2.81 ms | \\- | 2.89 ms|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Check the computational graph of quantized model\n",
    "\n",
    "## 1\\) Node category\n",
    "| Aspect        | Initializer                           | Constant node                          |\n",
    "| ------------- | ------------------------------------- | -------------------------------------- |\n",
    "| **Purpose**   | Model parameters (weights, biases)    | Inline constants inside ops            |\n",
    "| **Stored in** | `graph.initializer`                   | `graph.node` with `op_type=\"Constant\"` |\n",
    "| **Changes?**  | Can be updated (e.g., for finetuning) | Always fixed in graph logic            |\n",
    "| **Usage in our graph**     | generated in ort quantization                  | if it's directly exported from a quantized pytorch model then it's constant |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in initializer: `onnx::Conv_55_quantized`.\n",
      "  Shape: (32,)\n",
      "  Dtype: int32\n",
      "Found in initializer: `onnx::Conv_55_quantized_scale`.\n",
      "  Shape: (32,)\n",
      "  Dtype: float32\n",
      "Found /block1/block/0/Constant_6_output_0 in Constant node output `/block1/block/0/Constant_6`. \n",
      "  Shape: (32,)\n",
      "  Dtype: int32\n",
      "Found /block1/block/0/Constant_7_output_0 in Constant node output `/block1/block/0/Constant_7`. \n",
      "  Shape: (32,)\n",
      "  Dtype: float32\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "\n",
    "onnx_static = onnx.load(\"../models/cnn_int8.onnx\")\n",
    "o_graph = onnx_static.graph\n",
    "o_node = o_graph.node\n",
    "\n",
    "torch_ptq = onnx.load(\"../models/cnn_ptq.onnx\")\n",
    "t_graph = torch_ptq.graph\n",
    "t_node = t_graph.node\n",
    "\n",
    "def print_tensor_info(graph, tensor_name):\n",
    "    found = False\n",
    "\n",
    "    # Check in initializers\n",
    "    for init in graph.initializer:\n",
    "        if init.name == tensor_name:\n",
    "            val_array = onnx.numpy_helper.to_array(init)\n",
    "            print(f\"Found in initializer: `{tensor_name}`.\")\n",
    "            print(f\"  Shape: {val_array.shape}\")\n",
    "            print(f\"  Dtype: {val_array.dtype}\")\n",
    "            # print(f\"  Values: {val_array}\")\n",
    "            found = True\n",
    "            return val_array\n",
    "\n",
    "    # Check in Constant nodes\n",
    "    for node in graph.node:\n",
    "        if node.op_type == \"Constant\":\n",
    "            for attr in node.attribute:\n",
    "                if attr.name == \"value\":\n",
    "                    val_array = onnx.numpy_helper.to_array(attr.t)\n",
    "                    if node.output[0] == tensor_name:\n",
    "                        print(f\"Found {tensor_name} in Constant node output `{node.name}`. \")\n",
    "                        print(f\"  Shape: {val_array.shape}\")\n",
    "                        print(f\"  Dtype: {val_array.dtype}\")\n",
    "                        # print(f\"  Values: {val_array}\")\n",
    "                        found = True\n",
    "                        return val_array\n",
    "\n",
    "    if not found:\n",
    "        print(f\"Tensor '{tensor_name}' not found in initializers or Constant nodes.\")\n",
    "\n",
    "o_L1_bias_q = print_tensor_info(o_graph, \"onnx::Conv_55_quantized\")\n",
    "o_L1_bias_scale = print_tensor_info(o_graph, \"onnx::Conv_55_quantized_scale\")\n",
    "# o_L1_bias_zero_point is all-zero since it's symmetric quantization\n",
    "t_L1_bias_q = print_tensor_info(t_graph, \"/block1/block/0/Constant_6_output_0\")\n",
    "t_L1_bias_scale = print_tensor_info(t_graph, \"/block1/block/0/Constant_7_output_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AssertionError: \n",
      "Not equal to tolerance rtol=0.001, atol=1e-05\n",
      "\n",
      "Mismatched elements: 28 / 32 (87.5%)\n",
      "Max absolute difference among violations: 0.0007678\n",
      "Max relative difference among violations: 0.10642903\n",
      " ACTUAL: array([-0.00612 ,  0.028869,  0.016809, -0.003452,  0.004411, -0.001444,\n",
      "       -0.02042 ,  0.092428,  0.022525,  0.006155, -0.019724, -0.005085,\n",
      "       -0.00049 , -0.005574, -0.006422,  0.060337, -0.330705, -0.006446,...\n",
      " DESIRED: array([-0.006085,  0.028701,  0.016548, -0.00312 ,  0.004134, -0.001305,\n",
      "       -0.020278,  0.092147,  0.022479,  0.005934, -0.019425, -0.004903,\n",
      "        0.      , -0.005568, -0.006366,  0.06011 , -0.33066 , -0.006397,...\n"
     ]
    }
   ],
   "source": [
    "o_L1_bias_fp = o_L1_bias_q * o_L1_bias_scale\n",
    "t_L1_bias_fp = t_L1_bias_q * t_L1_bias_scale\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    np.testing.assert_allclose(o_L1_bias_fp, t_L1_bias_fp, rtol=1e-3, atol=1e-5)\n",
    "except AssertionError as e:\n",
    "    print(f\"AssertionError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in initializer: `onnx::Conv_54_quantized`.\n",
      "  Shape: (32, 1, 80)\n",
      "  Dtype: int8\n",
      "Found in initializer: `onnx::Conv_54_scale`.\n",
      "  Shape: (32,)\n",
      "  Dtype: float32\n",
      "Found /block1/block/0/Constant_2_output_0 in Constant node output `/block1/block/0/Constant_2`. \n",
      "  Shape: (32, 1, 80)\n",
      "  Dtype: int8\n",
      "Found /block1/block/0/Constant_3_output_0 in Constant node output `/block1/block/0/Constant_3`. \n",
      "  Shape: (32,)\n",
      "  Dtype: float32\n",
      "AssertionError: \n",
      "Not equal to tolerance rtol=0.001, atol=1e-05\n",
      "\n",
      "Mismatched elements: 2547 / 2560 (99.5%)\n",
      "Max absolute difference among violations: 0.11737084\n",
      "Max relative difference among violations: 1.\n",
      " ACTUAL: array([[[-0.966245,  0.191236,  0.462993, ..., -0.644163, -0.191236,\n",
      "         -0.744814]],\n",
      "...\n",
      " DESIRED: array([[[-0.962456,  0.200512,  0.461177, ..., -0.641637, -0.190486,\n",
      "         -0.741893]],\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "o_L1_weight_q = print_tensor_info(o_graph, \"onnx::Conv_54_quantized\")\n",
    "o_L1_weight_scale = print_tensor_info(o_graph, \"onnx::Conv_54_scale\")\n",
    "\n",
    "t_L1_weight_q = print_tensor_info(t_graph, \"/block1/block/0/Constant_2_output_0\")\n",
    "t_L1_weight_scale = print_tensor_info(t_graph, \"/block1/block/0/Constant_3_output_0\")\n",
    "\n",
    "o_L1_weight_fp = o_L1_weight_scale[:, np.newaxis, np.newaxis] * (o_L1_weight_q.astype(np.float32))\n",
    "t_L1_weight_fp = t_L1_weight_scale[:, np.newaxis, np.newaxis] * (t_L1_weight_q.astype(np.float32))\n",
    "try:\n",
    "    np.testing.assert_allclose(o_L1_weight_fp, t_L1_weight_fp, rtol=1e-3, atol=1e-5)\n",
    "except AssertionError as e:\n",
    "    print(f\"AssertionError: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in initializer: `onnx::Conv_58_quantized`.\n",
      "  Shape: (32,)\n",
      "  Dtype: int32\n",
      "Found in initializer: `onnx::Conv_58_quantized_scale`.\n",
      "  Shape: (32,)\n",
      "  Dtype: float32\n",
      "Found /block2/block/0/Constant_6_output_0 in Constant node output `/block2/block/0/Constant_6`. \n",
      "  Shape: (32,)\n",
      "  Dtype: int32\n",
      "Found /block2/block/0/Constant_7_output_0 in Constant node output `/block2/block/0/Constant_7`. \n",
      "  Shape: (32,)\n",
      "  Dtype: float32\n",
      "AssertionError: \n",
      "Not equal to tolerance rtol=0.001, atol=1e-05\n",
      "\n",
      "Mismatched elements: 9 / 32 (28.1%)\n",
      "Max absolute difference among violations: 0.00045552\n",
      "Max relative difference among violations: 0.0534982\n",
      " ACTUAL: array([-0.025805,  0.322926, -0.060563,  0.073844,  0.516396,  0.20104 ,\n",
      "        0.500617,  0.469001,  0.395666,  0.270268,  0.501922,  0.298508,\n",
      "        0.240467,  0.251023,  0.522687,  0.287385,  0.318088,  0.00897 ,...\n",
      " DESIRED: array([-0.025711,  0.323145, -0.060501,  0.07353 ,  0.516634,  0.200726,\n",
      "        0.500608,  0.468857,  0.395571,  0.270032,  0.501805,  0.298209,\n",
      "        0.240706,  0.250953,  0.522615,  0.286973,  0.318001,  0.008515,...\n"
     ]
    }
   ],
   "source": [
    "o_L2_bias_q = print_tensor_info(o_graph, \"onnx::Conv_58_quantized\")\n",
    "o_L2_bias_scale = print_tensor_info(o_graph, \"onnx::Conv_58_quantized_scale\")\n",
    "# o_L2_bias_zero_point is all-zero since it's symmetric quantization\n",
    "t_L2_bias_q = print_tensor_info(t_graph, \"/block2/block/0/Constant_6_output_0\")\n",
    "t_L2_bias_scale = print_tensor_info(t_graph, \"/block2/block/0/Constant_7_output_0\")\n",
    "\n",
    "o_L2_bias_fp = o_L2_bias_q * o_L2_bias_scale\n",
    "t_L2_bias_fp = t_L2_bias_q * t_L2_bias_scale\n",
    "\n",
    "try:\n",
    "    np.testing.assert_allclose(o_L2_bias_fp, t_L2_bias_fp, rtol=1e-3, atol=1e-5)\n",
    "except AssertionError as e:\n",
    "    print(f\"AssertionError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in initializer: `onnx::Conv_57_quantized`.\n",
      "  Shape: (32, 32, 3)\n",
      "  Dtype: int8\n",
      "Found in initializer: `onnx::Conv_57_scale`.\n",
      "  Shape: (32,)\n",
      "  Dtype: float32\n",
      "Found /block2/block/0/Constant_2_output_0 in Constant node output `/block2/block/0/Constant_2`. \n",
      "  Shape: (32, 32, 3)\n",
      "  Dtype: int8\n",
      "Found /block2/block/0/Constant_3_output_0 in Constant node output `/block2/block/0/Constant_3`. \n",
      "  Shape: (32,)\n",
      "  Dtype: float32\n",
      "AssertionError: \n",
      "Not equal to tolerance rtol=0.001, atol=1e-05\n",
      "\n",
      "Mismatched elements: 2816 / 3072 (91.7%)\n",
      "Max absolute difference among violations: 0.00203356\n",
      "Max relative difference among violations: 0.16338585\n",
      " ACTUAL: array([[[ 0.032159,  0.041882,  0.041134],\n",
      "        [ 0.08152 ,  0.044873,  0.051604],\n",
      "        [-0.011966,  0.020193,  0.055344],...\n",
      " DESIRED: array([[[ 0.032778,  0.041717,  0.040972],\n",
      "        [ 0.0812  ,  0.045442,  0.052147],\n",
      "        [-0.011919,  0.020114,  0.055126],...\n"
     ]
    }
   ],
   "source": [
    "o_L2_weight_q = print_tensor_info(o_graph, \"onnx::Conv_57_quantized\")\n",
    "o_L2_weight_scale = print_tensor_info(o_graph, \"onnx::Conv_57_scale\")\n",
    "\n",
    "t_L2_weight_q = print_tensor_info(t_graph, \"/block2/block/0/Constant_2_output_0\")\n",
    "t_L2_weight_scale = print_tensor_info(t_graph, \"/block2/block/0/Constant_3_output_0\")\n",
    "\n",
    "o_L2_weight_fp = o_L2_weight_scale[:, np.newaxis, np.newaxis] * (o_L2_weight_q.astype(np.float32))\n",
    "t_L2_weight_fp = t_L2_weight_scale[:, np.newaxis, np.newaxis] * (t_L2_weight_q.astype(np.float32))\n",
    "try:\n",
    "    np.testing.assert_allclose(o_L2_weight_fp, t_L2_weight_fp, rtol=1e-3, atol=1e-5)\n",
    "except AssertionError as e:\n",
    "    print(f\"AssertionError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in initializer: `onnx::Conv_61_quantized`.\n",
      "  Shape: (64,)\n",
      "  Dtype: int32\n",
      "Found in initializer: `onnx::Conv_61_quantized_scale`.\n",
      "  Shape: (64,)\n",
      "  Dtype: float32\n",
      "Found /block3/block/0/Constant_6_output_0 in Constant node output `/block3/block/0/Constant_6`. \n",
      "  Shape: (64,)\n",
      "  Dtype: int32\n",
      "Found /block3/block/0/Constant_7_output_0 in Constant node output `/block3/block/0/Constant_7`. \n",
      "  Shape: (64,)\n",
      "  Dtype: float32\n",
      "AssertionError: \n",
      "Not equal to tolerance rtol=0.001, atol=1e-05\n",
      "\n",
      "Mismatched elements: 14 / 64 (21.9%)\n",
      "Max absolute difference among violations: 0.0010885\n",
      "Max relative difference among violations: 0.04639256\n",
      " ACTUAL: array([ 1.016544,  0.211402,  1.294162,  1.235873,  0.696388,  0.216166,\n",
      "        0.591279,  0.390996,  0.13884 , -0.891073,  1.167723, -1.398085,\n",
      "        0.161216,  1.517577,  0.353284, -0.489279,  0.181339, -0.009228,...\n",
      " DESIRED: array([ 1.016257,  0.211391,  1.294079,  1.235603,  0.695967,  0.215784,\n",
      "        0.590768,  0.390695,  0.138811, -0.891067,  1.167422, -1.397726,\n",
      "        0.161151,  1.517263,  0.353275, -0.488969,  0.181011, -0.00908 ,...\n"
     ]
    }
   ],
   "source": [
    "o_L3_bias_q = print_tensor_info(o_graph, \"onnx::Conv_61_quantized\")\n",
    "o_L3_bias_scale = print_tensor_info(o_graph, \"onnx::Conv_61_quantized_scale\")\n",
    "t_L3_bias_q = print_tensor_info(t_graph, \"/block3/block/0/Constant_6_output_0\")\n",
    "t_L3_bias_scale = print_tensor_info(t_graph, \"/block3/block/0/Constant_7_output_0\")\n",
    "\n",
    "o_L3_bias_fp = o_L3_bias_q * o_L3_bias_scale\n",
    "t_L3_bias_fp = t_L3_bias_q * t_L3_bias_scale\n",
    "\n",
    "try:\n",
    "    np.testing.assert_allclose(o_L3_bias_fp, t_L3_bias_fp, rtol=1e-3, atol=1e-5)\n",
    "except AssertionError as e:\n",
    "    print(f\"AssertionError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in initializer: `onnx::Conv_60_quantized`.\n",
      "  Shape: (64, 32, 3)\n",
      "  Dtype: int8\n",
      "Found in initializer: `onnx::Conv_60_scale`.\n",
      "  Shape: (64,)\n",
      "  Dtype: float32\n",
      "Found /block3/block/0/Constant_2_output_0 in Constant node output `/block3/block/0/Constant_2`. \n",
      "  Shape: (64, 32, 3)\n",
      "  Dtype: int8\n",
      "Found /block3/block/0/Constant_3_output_0 in Constant node output `/block3/block/0/Constant_3`. \n",
      "  Shape: (64,)\n",
      "  Dtype: float32\n",
      "AssertionError: \n",
      "Not equal to tolerance rtol=0.001, atol=1e-05\n",
      "\n",
      "Mismatched elements: 6053 / 6144 (98.5%)\n",
      "Max absolute difference among violations: 0.01690417\n",
      "Max relative difference among violations: 1.\n",
      " ACTUAL: array([[[-0.188505,  0.081516,  0.005095],\n",
      "        [-0.331158,  0.091705,  0.163032],\n",
      "        [-0.045853, -0.035663, -0.081516],...\n",
      " DESIRED: array([[[-0.192841,  0.081196,  0.005075],\n",
      "        [-0.334934,  0.091346,  0.162392],\n",
      "        [-0.045673, -0.035523, -0.081196],...\n"
     ]
    }
   ],
   "source": [
    "o_L3_weight_q = print_tensor_info(o_graph, \"onnx::Conv_60_quantized\")\n",
    "o_L3_weight_scale = print_tensor_info(o_graph, \"onnx::Conv_60_scale\")\n",
    "\n",
    "t_L3_weight_q = print_tensor_info(t_graph, \"/block3/block/0/Constant_2_output_0\")\n",
    "t_L3_weight_scale = print_tensor_info(t_graph, \"/block3/block/0/Constant_3_output_0\")\n",
    "\n",
    "o_L3_weight_fp = o_L3_weight_scale[:, np.newaxis, np.newaxis] * (o_L3_weight_q.astype(np.float32))\n",
    "t_L3_weight_fp = t_L3_weight_scale[:, np.newaxis, np.newaxis] * (t_L3_weight_q.astype(np.float32))\n",
    "try:\n",
    "    np.testing.assert_allclose(o_L3_weight_fp, t_L3_weight_fp, rtol=1e-3, atol=1e-5)\n",
    "except AssertionError as e:\n",
    "    print(f\"AssertionError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in initializer: `onnx::Conv_64_quantized`.\n",
      "  Shape: (64,)\n",
      "  Dtype: int32\n",
      "Found in initializer: `onnx::Conv_64_quantized_scale`.\n",
      "  Shape: (64,)\n",
      "  Dtype: float32\n",
      "Found /block4/block/0/Constant_6_output_0 in Constant node output `/block4/block/0/Constant_6`. \n",
      "  Shape: (64,)\n",
      "  Dtype: int32\n",
      "Found /block4/block/0/Constant_7_output_0 in Constant node output `/block4/block/0/Constant_7`. \n",
      "  Shape: (64,)\n",
      "  Dtype: float32\n",
      "AssertionError: \n",
      "Not equal to tolerance rtol=0.001, atol=1e-05\n",
      "\n",
      "Mismatched elements: 7 / 64 (10.9%)\n",
      "Max absolute difference among violations: 0.00041547\n",
      "Max relative difference among violations: 0.00333149\n",
      " ACTUAL: array([-0.317625,  1.4661  , -0.215446, -0.61548 , -1.684858, -1.574953,\n",
      "        1.452836, -0.67934 ,  0.495698, -0.343092, -2.538879, -2.524491,\n",
      "       -0.080531, -2.704656, -1.390812,  0.305723,  0.029642, -0.738222,...\n",
      " DESIRED: array([-0.317305,  1.466154, -0.21533 , -0.615283, -1.684831, -1.574783,\n",
      "        1.452739, -0.679174,  0.495546, -0.342677, -2.538717, -2.524513,\n",
      "       -0.080264, -2.704643, -1.390637,  0.305531,  0.029602, -0.738098,...\n"
     ]
    }
   ],
   "source": [
    "o_L4_bias_q = print_tensor_info(o_graph, \"onnx::Conv_64_quantized\")\n",
    "o_L4_bias_scale = print_tensor_info(o_graph, \"onnx::Conv_64_quantized_scale\")\n",
    "t_L4_bias_q = print_tensor_info(t_graph, \"/block4/block/0/Constant_6_output_0\")\n",
    "t_L4_bias_scale = print_tensor_info(t_graph, \"/block4/block/0/Constant_7_output_0\")\n",
    "\n",
    "o_L4_bias_fp = o_L4_bias_q * o_L4_bias_scale\n",
    "t_L4_bias_fp = t_L4_bias_q * t_L4_bias_scale\n",
    "\n",
    "try:\n",
    "    np.testing.assert_allclose(o_L4_bias_fp, t_L4_bias_fp, rtol=1e-3, atol=1e-5)\n",
    "except AssertionError as e:\n",
    "    print(f\"AssertionError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in initializer: `onnx::Conv_63_quantized`.\n",
      "  Shape: (64, 64, 3)\n",
      "  Dtype: int8\n",
      "Found in initializer: `onnx::Conv_63_scale`.\n",
      "  Shape: (64,)\n",
      "  Dtype: float32\n",
      "Found /block4/block/0/Constant_2_output_0 in Constant node output `/block4/block/0/Constant_2`. \n",
      "  Shape: (64, 64, 3)\n",
      "  Dtype: int8\n",
      "Found /block4/block/0/Constant_3_output_0 in Constant node output `/block4/block/0/Constant_3`. \n",
      "  Shape: (64,)\n",
      "  Dtype: float32\n",
      "AssertionError: \n",
      "Not equal to tolerance rtol=0.001, atol=1e-05\n",
      "\n",
      "Mismatched elements: 12118 / 12288 (98.6%)\n",
      "Max absolute difference among violations: 0.01933029\n",
      "Max relative difference among violations: 1.\n",
      " ACTUAL: array([[[ 0.264763, -0.63249 ,  0.500108],\n",
      "        [ 0.102963,  0.411854, -1.073762],\n",
      "        [-0.294181,  0.      ,  0.102963],...\n",
      " DESIRED: array([[[ 0.263725, -0.644661,  0.498147],\n",
      "        [ 0.10256 ,  0.410239, -1.069551],\n",
      "        [-0.293028,  0.      ,  0.10256 ],...\n"
     ]
    }
   ],
   "source": [
    "o_L4_weight_q = print_tensor_info(o_graph, \"onnx::Conv_63_quantized\")\n",
    "o_L4_weight_scale = print_tensor_info(o_graph, \"onnx::Conv_63_scale\")\n",
    "\n",
    "t_L4_weight_q = print_tensor_info(t_graph, \"/block4/block/0/Constant_2_output_0\")\n",
    "t_L4_weight_scale = print_tensor_info(t_graph, \"/block4/block/0/Constant_3_output_0\")\n",
    "\n",
    "o_L4_weight_fp = o_L4_weight_scale[:, np.newaxis, np.newaxis] * (o_L4_weight_q.astype(np.float32))\n",
    "t_L4_weight_fp = t_L4_weight_scale[:, np.newaxis, np.newaxis] * (t_L4_weight_q.astype(np.float32))\n",
    "try:\n",
    "    np.testing.assert_allclose(o_L4_weight_fp, t_L4_weight_fp, rtol=1e-3, atol=1e-5)\n",
    "except AssertionError as e:\n",
    "    print(f\"AssertionError: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
