{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c1a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e . -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a1addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from src.utils import *\n",
    "from src.data_loader import get_data_loaders\n",
    "from src.models.cnn_model import M5, QATM5, PTQM5\n",
    "from src.models.cnn_model_LayerWiseQuant import M5Modular, PTQM5Modular, PTQM5_LayerWiseQuant, QATM5Modular, QATM5_LayerWiseQuant\n",
    "from src.train import set_seed, train_model\n",
    "from src.evaluate import evaluate_model, test\n",
    "\n",
    "with open(\"configs/cnn_ptq_LayerWiseQuant.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Get data\n",
    "train_loader, test_loader, validate_loader = get_data_loaders(config[\"data\"])\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model_config = config[\"model\"][\"base_cnn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63fec5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 35 35\n"
     ]
    }
   ],
   "source": [
    "num_classes_train = len(set(label for _, _, label, *_ in train_loader.dataset))\n",
    "num_classes_test = len(set(label for _, _, label, *_ in test_loader.dataset))\n",
    "num_classes_validate = len(set(label for _, _, label, *_ in validate_loader.dataset))\n",
    "print(num_classes_train, num_classes_test, num_classes_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c8c8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train_loader.dataset: 84843\n",
      "length of test_loader.dataset: 11005\n",
      "length of validate_loader.dataset: 9981\n"
     ]
    }
   ],
   "source": [
    "print(\"length of train_loader.dataset:\", len(train_loader.dataset))\n",
    "print(\"length of test_loader.dataset:\", len(test_loader.dataset))\n",
    "print(\"length of validate_loader.dataset:\", len(validate_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ea6ef80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples across all datasets: 105829\n",
      "ratio of train to test to validate datasets: 0.8016989672018067 0.10398850976575419 0.09431252303243913\n"
     ]
    }
   ],
   "source": [
    "n_all = len(train_loader.dataset) + len(test_loader.dataset) + len(validate_loader.dataset)\n",
    "print(\"Total number of samples across all datasets:\", n_all)\n",
    "print(\"ratio of train to test to validate datasets:\",\n",
    "      len(train_loader.dataset) / n_all, len(test_loader.dataset) / n_all, len(validate_loader.dataset) / n_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59aade61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model QAT config: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "Activations: functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, reduce_range=True){}\n",
      "Weights: functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){}\n"
     ]
    }
   ],
   "source": [
    "model_qat = QATM5Modular(\n",
    "            n_input=model_config[\"n_input\"],\n",
    "            n_output=model_config[\"n_output\"],\n",
    "            stride=model_config[\"stride\"],\n",
    "            n_channel=model_config[\"n_channel\"],\n",
    "            conv_kernel_sizes=model_config[\"conv_kernel_sizes\"]\n",
    "        )\n",
    "\n",
    "model_qat.eval()\n",
    "model_qat.fuse_model()\n",
    "model_qat.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n",
    "print(\"Model QAT config:\", model_qat.qconfig)\n",
    "print(\"Activations:\", model_qat.qconfig.activation)\n",
    "# print(\"Activations Schemes:\", model_qat.qconfig.activation().scheme)\n",
    "print(\"Weights:\", model_qat.qconfig.weight)\n",
    "# print(\"Weights Schemes:\", model_qat.qconfig.weight().scheme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec8b414c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}\n",
      "functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){}\n",
      "torch.per_tensor_affine\n",
      "torch.per_channel_symmetric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/audioml/lib/python3.13/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# set_seed(config[\"model\"][\"base_cnn\"][\"seed\"])\n",
    "\n",
    "# Load FP32 model\n",
    "model_fp32 = PTQM5Modular(n_input=model_config[\"n_input\"],\n",
    "            n_output=model_config[\"n_output\"],\n",
    "            stride=model_config[\"stride\"],\n",
    "            n_channel=model_config[\"n_channel\"],\n",
    "            conv_kernel_sizes=model_config[\"conv_kernel_sizes\"]).to('cpu')\n",
    "model_fp32.eval()\n",
    "model_fp32.load_state_dict(torch.load(model_config[\"pretrained_path\"]))\n",
    "model_fp32.fuse_model()\n",
    "model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "\n",
    "print(model_fp32.qconfig.activation)\n",
    "print(model_fp32.qconfig.weight)\n",
    "act_obs = model_fp32.qconfig.activation()\n",
    "wt_obs = model_fp32.qconfig.weight()\n",
    "\n",
    "print(act_obs.qscheme)\n",
    "print(wt_obs.qscheme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958a7f4e",
   "metadata": {},
   "source": [
    "| No. | Activation Observer                     | Weight Observer                             | Details                                      |\n",
    "|-----|-----------------------------------------|---------------------------------------------|----------------------------------------------|\n",
    "| 01  | HistogramObserver                       | PerChannelMinMaxObserver                    | Default                                       |\n",
    "| 02  | HistogramObserver                       | MovingAveragePerChannelMinMaxObserver       |                                              |\n",
    "| 03  | HistogramObserver                       | MinMaxObserver                              |                                              |\n",
    "| 04  | HistogramObserver                       | MovingAverageMinMaxObserver                 |                                              |\n",
    "| 05  | HistogramObserver                       | HistogramObserver                           | Fully histogram-based                        |\n",
    "| 06  | MinMaxObserver                          | PerChannelMinMaxObserver                    | Fastest; not robust to outliers              |\n",
    "| 07  | MinMaxObserver                          | MovingAveragePerChannelMinMaxObserver       |                                              |\n",
    "| 08  | MinMaxObserver                          | MinMaxObserver                              |                                              |\n",
    "| 09  | MinMaxObserver                          | MovingAverageMinMaxObserver                 |                                              |\n",
    "| 10  | MinMaxObserver                          | HistogramObserver                           |                                              |\n",
    "| 11  | MovingAverageMinMaxObserver             | PerChannelMinMaxObserver                    | Balanced and smoother                        |\n",
    "| 12  | MovingAverageMinMaxObserver             | MovingAveragePerChannelMinMaxObserver       | Recommended for most conv nets               |\n",
    "| 13  | MovingAverageMinMaxObserver             | MinMaxObserver                              |                                              |\n",
    "| 14  | MovingAverageMinMaxObserver             | MovingAverageMinMaxObserver                 |                                              |\n",
    "| 15  | MovingAverageMinMaxObserver             | HistogramObserver                           |                                              |\n",
    "| 16  | HistogramObserver (Entropy/MSE)         | PerChannelMinMaxObserver                    | Use Entropy/MSE clipping for activation      |\n",
    "| 17  | HistogramObserver (Entropy/MSE)         | MovingAveragePerChannelMinMaxObserver       |                                              |\n",
    "| 18  | HistogramObserver (Percentile Clipping) | PerChannelMinMaxObserver                    | Specify percentile like 99.9%                |\n",
    "| 19  | HistogramObserver (Percentile Clipping) | MovingAveragePerChannelMinMaxObserver       |                                              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.observer import (\n",
    "    HistogramObserver,\n",
    "    PerChannelMinMaxObserver,\n",
    "    MovingAveragePerChannelMinMaxObserver\n",
    ")\n",
    "from torch.ao.quantization import QConfig\n",
    "\n",
    "# Example: Use HistogramObserver for activations, PerChannelMinMaxObserver for weights\n",
    "qconfig_movingAvgPerChannelMinMax = QConfig(\n",
    "    activation=HistogramObserver.with_args(reduce_range=True),\n",
    "    weight=MovingAveragePerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric)\n",
    ")\n",
    "model_movingAvgPerChannelMinMax = model_fp32.deepcopy()\n",
    "model_movingAvgPerChannelMinMax.qconfig = qconfig_movingAvgPerChannelMinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c367b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ao.quantization.prepare(model_fp32, inplace=True)\n",
    "\n",
    "# Calibrate model - use validation set\n",
    "with torch.inference_mode():\n",
    "    for data, _ in validate_loader:\n",
    "        data = data.to(\"cpu\")\n",
    "        model_fp32(data)\n",
    "\n",
    "# Convert to PTQ model\n",
    "model = torch.ao.quantization.convert(model_fp32, inplace=False)\n",
    "print(test(model, train_loader))\n",
    "print(test(model, test_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
